{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lincolnschick/ML4MC/blob/main/main/docs/reports/requirement-38/fh_combat_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "eDMQY4kQ-vxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables to change based upon specifics of test\n",
        "TEST_EPISODES = 1 # Number of tests to run for each model\n",
        "TEST_STEPS = 10000 # Total timesteps to run for each model\n",
        "USING_CUSTOM_ENV = True #Are we using a custom enviroment\n",
        "DIRECTORY_PATH = \"/content/drive/MyDrive/packages/minerl_saved_models\" #Directory we have the models saved in\n",
        "SAVE_LOCATION = \"/content/drive/MyDrive/packages/minerl_test_outputs\" #Directory we are saving videos to\n",
        "FORCE_STOP = False #Force stops after one test (for code testing purposes)"
      ],
      "metadata": {
        "id": "soJAOH11JLDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installations"
      ],
      "metadata": {
        "id": "FogkrTGbACWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "# Allow colab to access google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0jWXGqATTyUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4804388-0d56-4ced-8636-3dbf3d652675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_MINERL = True\n",
        "\n",
        "if LOCAL_MINERL:\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/minerl\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/minerl\")\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\")\n"
      ],
      "metadata": {
        "id": "NmHH-8bnk6Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt install tigervnc-standalone-server\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip3 install gym==0.13.1\n",
        "if LOCAL_MINERL:\n",
        "  !pip3 install -e /content/drive/MyDrive/packages/minerl\n",
        "else:\n",
        "  !pip3 install minerl==0.4.4 --verbose\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!sudo apt-get install xvfb\n",
        "!pip3 install opencv-python\n",
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "3EflRXDLT0uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Environment Setup"
      ],
      "metadata": {
        "id": "3U6bqwuFAKWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minerl.herobraine.env_specs.simple_embodiment import SimpleEmbodimentEnvSpec\n",
        "from minerl.herobraine.hero.handler import Handler\n",
        "from typing import List\n",
        "import random\n",
        "\n",
        "import minerl.herobraine.hero.handlers as handlers\n",
        "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The intent of this env_spec is to create a survival environment for our agent to be evaluated in.\n",
        "This environment allows us to tailor the observation and action spaces to our agent's and UI's needs.\n",
        "\"\"\"\n",
        "\n",
        "NONE = 'none'\n",
        "OTHER = 'other'\n",
        "\n",
        "MS_PER_STEP = 50\n",
        "\n",
        "ML4MC_SURVIVAL_LENGTH = 1 * 60 * 60 * 20  # 1 hour * 60 minutes * 60 seconds * 20 ticks/steps per second\n",
        "\n",
        "class ML4MCSurvival(SimpleEmbodimentEnvSpec):\n",
        "    # ML4MCSurvival constructor\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if 'name' not in kwargs:\n",
        "            kwargs['name'] = 'ML4MCSurvival-v0' # Add environment name if not added\n",
        "\n",
        "        super().__init__(*args, max_episode_steps=ML4MC_SURVIVAL_LENGTH, **kwargs)\n",
        "\n",
        "    # Allows scripts to observe inventory, equipped item, and current location related stats\n",
        "    def create_observables(self) -> List[Handler]:\n",
        "        return super().create_observables() + [\n",
        "            handlers.FlatInventoryObservation(ALL_ITEMS),\n",
        "            handlers.EquippedItemObservation(items=[\n",
        "                'air', 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe', NONE,\n",
        "                OTHER\n",
        "            ], _default='air', _other=OTHER),\n",
        "            handlers.ObservationFromCurrentLocation(),\n",
        "            handlers.ObservationFromLifeStats(),\n",
        "        ]\n",
        "\n",
        "    # Allows scripts to place blocks, equip items, craft items, and smelt items\n",
        "    def create_actionables(self):\n",
        "        return super().create_actionables() + [\n",
        "            handlers.PlaceBlock([NONE, 'dirt', 'stone', 'cobblestone', 'crafting_table', 'furnace', 'torch'],\n",
        "                                _other=NONE, _default=NONE),\n",
        "            handlers.EquipAction([NONE, 'air', 'wooden_axe', 'stone_axe', 'iron_axe', 'stone_sword', 'iron_sword', 'wooden_sword'], _other=NONE, _default=NONE),\n",
        "            handlers.CraftAction([NONE, 'torch', 'stick', 'planks', 'crafting_table'], _other=NONE, _default=NONE),\n",
        "            handlers.CraftNearbyAction(\n",
        "                [NONE, 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe',\n",
        "                 'furnace'], _other=NONE, _default=NONE),\n",
        "            handlers.SmeltItemNearby([NONE, 'iron_ingot', 'coal'], _other=NONE, _default=NONE),\n",
        "        ]\n",
        "\n",
        "    # Rewards for collecting iron (and cobblestone)\n",
        "    def create_rewardables(self) -> List[Handler]:\n",
        "        return [\n",
        "            # handlers.RewardForCollectingItems([\n",
        "            #     dict(type=\"cobblestone\", amount=1, reward=256.0),\n",
        "            #     dict(type=\"dirt\", amount=1, reward=64.0),\n",
        "            # ])\n",
        "            handlers.RewardForXPGain(\n",
        "                reward_per_xp=100.0,\n",
        "                reward_type=\"FIXED\"),\n",
        "            handlers.ConstantReward(constant=1.0)\n",
        "            # handlers.RewardForTakingDMG(\n",
        "            #     reward_per_dmg=10.0,\n",
        "            #     reward_type=\"FIXED\")\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Start the agent with nothing by default, can be modified for testing\n",
        "    def create_agent_start(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.SimpleInventoryAgentStart([\n",
        "                dict(type=\"iron_sword\", quantity=5)\n",
        "            ])\n",
        "        ]\n",
        "\n",
        "    # No agent handlers needed as we are not using any rewards\n",
        "    def create_agent_handlers(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.AgentQuitFromPossessingItem([\n",
        "                dict(type=\"diamond_ore\", amount=32)]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Use the default world generator\n",
        "    def create_server_world_generators(self) -> List[Handler]:\n",
        "        # return [handlers.BiomeGenerator(\"extreme_hills\")]\n",
        "        return [\n",
        "            handlers.FlatWorldGenerator(generatorString=\"3;7,220*1,5*3,2;3;dungeon\")\n",
        "            # handlers.DrawEntityHandler(\n",
        "            #     xpos=str(random.randint(0,100)/10),\n",
        "            #     ypos=str(random.randint(0,100)/10),\n",
        "            #     zpos=str(random.randint(0,100)/10),\n",
        "            #     mobname=\"zombie\"\n",
        "            # )\n",
        "\n",
        "        ]\n",
        "\n",
        "    def create_server_quit_producers(self) -> List[Handler]:\n",
        "        # Set a timeout to end the episode to prevent it from running forever\n",
        "        return [\n",
        "            handlers.ServerQuitFromTimeUp(time_limit_ms=self.max_episode_steps * MS_PER_STEP),\n",
        "            handlers.ServerQuitWhenAnyAgentFinishes()\n",
        "        ]\n",
        "\n",
        "    # This method can be used to change other things about the world such as drawing shapes or spawning a village\n",
        "    # Not needed for ML4MCSurvival\n",
        "    def create_server_decorators(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.DrawingDecorator(\"\"\"\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"16\" y1=\"1\" y2=\"50\" z1=\"-15\" z2=\"16\" />\n",
        "              <DrawCuboid type=\"air\" x1=\"-12\" x2=\"13\" y1=\"4\" y2=\"39\" z1=\"-12\" z2=\"13\" />\n",
        "              <DrawCuboid type=\"glowstone\" x1=\"-14\" x2=\"15\" y1=\"7\" y2=\"30\" z1=\"-14\" z2=\"15\" />\n",
        "            \"\"\")\n",
        "            # handlers.DrawEntityHandler(\n",
        "            #     xpos=str(random.randint(0,100)/10),\n",
        "            #     ypos=str(random.randint(0,100)/10),\n",
        "            #     zpos=str(random.randint(0,100)/10),\n",
        "            #     mobname=\"zombie\"\n",
        "            # )\n",
        "        ]\n",
        "\n",
        "    # This method sets the conditions for the world the agent will spawn into\n",
        "    # We will allow spawning and the passage of time to replicate a realistic Minecraft environment\n",
        "    def create_server_initial_conditions(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.TimeInitialCondition(\n",
        "                start_time=18000, #18000 is night #6000 is day\n",
        "                allow_passage_of_time=True,\n",
        "            ),\n",
        "            handlers.SpawningInitialCondition(\n",
        "                allow_spawning=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def is_from_folder(self, folder: str) -> bool:\n",
        "        return folder == 'ml4mc_survival'\n",
        "\n",
        "    # Don't need docstring as we're not publishing this environment to MineRL's website\n",
        "    def get_docstring(self):\n",
        "        return \"\"\n",
        "\n",
        "    def determine_success_from_rewards(self, rewards: list) -> bool:\n",
        "        # All survival experiemnts are a success =)\n",
        "        return sum(rewards) >= self.reward_threshold"
      ],
      "metadata": {
        "id": "pQ0lkOviQdEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "IGVuu37eAxGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import gym\n",
        "import minerl\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging\n",
        "logging.disable(logging.ERROR)\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "mKJnAaPmTp7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start the Display for saving videos on Colab\n",
        "from pyvirtualdisplay import Display\n",
        "from os import path\n",
        "display = Display(visible=False, size=(400, 300))\n",
        "display.start();"
      ],
      "metadata": {
        "id": "J8ZrxqPoWiZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ],
      "metadata": {
        "id": "p7Gl5a51N0lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionShaping(gym.ActionWrapper):\n",
        "  def __init__(self, env, camera_angle=10, always_attack=False):\n",
        "    super().__init__(env)\n",
        "\n",
        "    self.camera_angle = camera_angle\n",
        "    self.always_attack = always_attack\n",
        "    self._actions = [\n",
        "      [('attack', 1)],\n",
        "      [('forward', 1)],\n",
        "      # [('back', 1)],\n",
        "      # [('left', 1)],\n",
        "      # [('right', 1)],\n",
        "      # [('jump', 1)],\n",
        "      # [('forward', 1), ('attack', 1)],\n",
        "      # [('craft', 'planks')],\n",
        "      [('forward', 1), ('jump', 1)],\n",
        "      [('camera', [-self.camera_angle, 0])],\n",
        "      [('camera', [self.camera_angle, 0])],\n",
        "      [('camera', [0, self.camera_angle])],\n",
        "      [('camera', [0, -self.camera_angle])],\n",
        "    ]\n",
        "\n",
        "    self.actions = []\n",
        "    for actions in self._actions:\n",
        "      act = self.env.action_space.noop()\n",
        "      for a, v in actions:\n",
        "        act[a] = v\n",
        "      if self.always_attack:\n",
        "        act['attack'] = 1\n",
        "      self.actions.append(act)\n",
        "\n",
        "    self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "  def action(self, action):\n",
        "    return self.actions[action]"
      ],
      "metadata": {
        "id": "RRfm-fLaORlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
        "  # There are dummy dimensions of shape one\n",
        "  camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "  attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "  forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "  jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "  batch_size = len(camera_actions)\n",
        "  actions = np.zeros((batch_size,), dtype=np.int)\n",
        "\n",
        "  for i in range(len(camera_actions)):\n",
        "    # Moving camera is most important (horizontal first)\n",
        "    if camera_actions[i][0] < -camera_margin:\n",
        "      actions[i] = 3\n",
        "    elif camera_actions[i][0] > camera_margin:\n",
        "      actions[i] = 4\n",
        "    elif camera_actions[i][1] > camera_margin:\n",
        "      actions[i] = 5\n",
        "    elif camera_actions[i][1] < -camera_margin:\n",
        "      actions[i] = 6\n",
        "    elif forward_actions[i] == 1:\n",
        "      if jump_actions[i] == 1:\n",
        "        actions[i] = 2\n",
        "      else:\n",
        "        actions[i] = 1\n",
        "    elif attack_actions[i] == 1:\n",
        "      actions[i] = 0\n",
        "    else:\n",
        "      # No reasonable mapping (would be no-op)\n",
        "      actions[i] = -1\n",
        "  return actions"
      ],
      "metadata": {
        "id": "ukMZ747COyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_act(env, actions):\n",
        "  act = env.action_space.noop()\n",
        "  for action in actions.split():\n",
        "    if \":\" in action:\n",
        "      k, v = action.split(':')\n",
        "      if k == 'camera':\n",
        "        act[k] = eval(v)\n",
        "      else:\n",
        "        act[k] = v\n",
        "    else:\n",
        "      act[action] = 1\n",
        "  return act"
      ],
      "metadata": {
        "id": "8M2RV4YXPFAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USING_CUSTOM_ENV:\n",
        "  abs_CUSTOM = ML4MCSurvival()\n",
        "  abs_CUSTOM.register()"
      ],
      "metadata": {
        "id": "8Qt-5m-ZNejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAqdZhEcwJwF",
        "outputId": "59b548ee-65b2-40c4-c85c-f384f9d3803b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 stable-baselines3-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "30lI29jkt2xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common import monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "metadata": {
        "id": "YDhtSIzlt2Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "  def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "    super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "    self.check_freq = check_freq\n",
        "    self.log_dir = log_dir\n",
        "    self.save_path = os.path.join(log_dir, 'best_model')\n",
        "    self.best_mean_reward = -np.inf\n",
        "\n",
        "  def _init_callack(self) -> None:\n",
        "    if self.save_path is not None:\n",
        "      os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "    if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "      #Retrieve  Training Reward\n",
        "      x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "      if len(x) > 0:\n",
        "          #Mean training reward over the last 100 episodes\n",
        "          mean_reward = np.mean(y[-100:])\n",
        "          if self.verbose > 0:\n",
        "            print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "            #New best model, save the agent\n",
        "            if mean_reward > self.best_mean_reward:\n",
        "              self.best_mean_reward = mean_reward\n",
        "              #Example for saving best model\n",
        "              if self.verbose > 0:\n",
        "                print(\"Saving new best model to {}\".format(self.save_path))\n",
        "              self.model.save(self.save_path)\n",
        "    return True"
      ],
      "metadata": {
        "id": "sS5wBnEZt6EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# abs_STONE = StoneCollection()\n",
        "# abs_STONE.register() # Register with gym"
      ],
      "metadata": {
        "id": "Zw8n3K0Ft_GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "if USING_CUSTOM_ENV:\n",
        "  env = gym.make('ML4MCSurvival-v0')\n",
        "else:\n",
        "  env = gym.make('MineRLObtainDiamond-v0')"
      ],
      "metadata": {
        "id": "XP5K4SSXuDPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, \"/content/drive/MyDrive/ml4mc_outputs\", fps=60)"
      ],
      "metadata": {
        "id": "WH-fkm76uHJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igJif-0kuJfo",
        "outputId": "7249b404-253f-4855-e96a-ba9f055fbfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A wrapper for getting the POV of the avatar from the environment, which is needed for stable_baselines\n",
        "class ExtractPOV(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    self.observation_space = self.env.observation_space['pov']\n",
        "\n",
        "  def observation(self, observation):\n",
        "    return observation['pov']"
      ],
      "metadata": {
        "id": "HfpLr5Kgy1fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#callback addition\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "monitored_env = monitor.Monitor(env1, log_dir)\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\"\"\"\n",
        "obs_wrapped_stone = ExtractPOV(env) #Extracting the POV of the avatar from the environment which is needed for stable_baselines\n",
        "obs_action_wrapped_stone = ActionShaping(obs_wrapped_stone) #Performing action shaping on the actions of the environment to convert them from dictionaries into an array.\n",
        "obs = obs_action_wrapped_stone.reset() #reseting the provided environnment\n",
        "\n",
        "\n",
        "\n",
        "model = PPO(policy=\"CnnPolicy\", env=obs_action_wrapped_stone, verbose=1) #Setting the model to be a PPO model with a CnnPolicy. This was just the model used by tutorials, we'll experiment with the best model later\n",
        "model.learn(total_timesteps=50000) #Training the model, allowing it to walk through 50000 timesteps of the environment (about 1.5 minutes)\n",
        "env.release() #releasing the recorded environment to actually make a video on Colab.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaSaJGnduMiw",
        "outputId": "344c5536-c80d-4235-dffb-a857ca76fec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 586      |\n",
            "|    ep_rew_mean     | 0        |\n",
            "| time/              |          |\n",
            "|    fps             | 8        |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 648         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 484         |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021744173 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.181       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.022      |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0226     |\n",
            "|    value_loss           | 0.00826     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 654         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 706         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021046083 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.89       |\n",
            "|    explained_variance   | 0.298       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00996     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    value_loss           | 0.00533     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 615         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 969         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015069437 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | -0.845      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0312     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    value_loss           | 0.0038      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 610         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 1193        |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015969168 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | -0.411      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0541     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    value_loss           | 0.00308     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 636         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 1417        |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019928027 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.85       |\n",
            "|    explained_variance   | -0.628      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0424     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0327     |\n",
            "|    value_loss           | 0.00223     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 629         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 1641        |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024127029 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | -0.369      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0437     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0338     |\n",
            "|    value_loss           | 0.00156     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 649        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 1882       |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04091153 |\n",
            "|    clip_fraction        | 0.299      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.84      |\n",
            "|    explained_variance   | -0.59      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0901    |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.0463    |\n",
            "|    value_loss           | 0.0013     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 655        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 9          |\n",
            "|    time_elapsed         | 2112       |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03048134 |\n",
            "|    clip_fraction        | 0.319      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.68      |\n",
            "|    explained_variance   | -1.05      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0457    |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | -0.0348    |\n",
            "|    value_loss           | 0.000611   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 672         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 2317        |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024698908 |\n",
            "|    clip_fraction        | 0.285       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | -0.737      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.065      |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.038      |\n",
            "|    value_loss           | 0.000761    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 667         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 2526        |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028216537 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.66       |\n",
            "|    explained_variance   | -1.8        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0525     |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0302     |\n",
            "|    value_loss           | 0.000455    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 680         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 2789        |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026013177 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | -0.594      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0792     |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0373     |\n",
            "|    value_loss           | 0.000491    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 683        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 2997       |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03761672 |\n",
            "|    clip_fraction        | 0.344      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.67      |\n",
            "|    explained_variance   | -3.12      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0349    |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.0441    |\n",
            "|    value_loss           | 0.000658   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 691         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 3240        |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040673874 |\n",
            "|    clip_fraction        | 0.332       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -2.97       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.1        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0504     |\n",
            "|    value_loss           | 0.000414    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 702        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 3446       |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03953845 |\n",
            "|    clip_fraction        | 0.36       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.68      |\n",
            "|    explained_variance   | -6.31      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0854    |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.0612    |\n",
            "|    value_loss           | 0.000349   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 696         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 3722        |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022924691 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | -3.84       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0434     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.034      |\n",
            "|    value_loss           | 0.000216    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 694         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 3957        |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039163217 |\n",
            "|    clip_fraction        | 0.328       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.66       |\n",
            "|    explained_variance   | -2.09       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0922     |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0541     |\n",
            "|    value_loss           | 0.000256    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 697        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 4157       |\n",
            "|    total_timesteps      | 36864      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04621023 |\n",
            "|    clip_fraction        | 0.37       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.67      |\n",
            "|    explained_variance   | -5.4       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0958    |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.0621    |\n",
            "|    value_loss           | 0.000252   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 697        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 4390       |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07095291 |\n",
            "|    clip_fraction        | 0.382      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.72      |\n",
            "|    explained_variance   | -6.17      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.088     |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0634    |\n",
            "|    value_loss           | 0.000227   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 712        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 4595       |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04480099 |\n",
            "|    clip_fraction        | 0.392      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.72      |\n",
            "|    explained_variance   | -4.92      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0859    |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0648    |\n",
            "|    value_loss           | 0.000169   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 712         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 4815        |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036264688 |\n",
            "|    clip_fraction        | 0.34        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | -9.84       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0824     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0412     |\n",
            "|    value_loss           | 0.000116    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 713         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 5020        |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035091892 |\n",
            "|    clip_fraction        | 0.391       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.71       |\n",
            "|    explained_variance   | -12.4       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0953     |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.06       |\n",
            "|    value_loss           | 9.35e-05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 730        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 9          |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 5233       |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04083658 |\n",
            "|    clip_fraction        | 0.396      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | -1.69      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.103     |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.0583    |\n",
            "|    value_loss           | 7.93e-05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 732         |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 8           |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 5473        |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.051613092 |\n",
            "|    clip_fraction        | 0.417       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.66       |\n",
            "|    explained_variance   | -2.42       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0614     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0693     |\n",
            "|    value_loss           | 8.11e-05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 728        |\n",
            "|    ep_rew_mean          | 0          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 8          |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 5714       |\n",
            "|    total_timesteps      | 51200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03841087 |\n",
            "|    clip_fraction        | 0.375      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.68      |\n",
            "|    explained_variance   | -3.03      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0959    |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.0588    |\n",
            "|    value_loss           | 5.49e-05   |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "results_plotter.plot_results([log_dir], 5000, results_plotter.X_TIMESTEPS, \"MineRL RL Training\")\n",
        "plt.show()\n",
        "\"\"\"\n",
        "model.save(DIRECTORY_PATH + \"/\" + 'combat.pth' )"
      ],
      "metadata": {
        "id": "aQeJHpnduOs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "4CiOo06Ft4cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# directory = os.fsencode(DIRECTORY_PATH)\n",
        "# network = NatureCNN((3, 64, 64), 7)\n",
        "\n",
        "# if USING_CUSTOM_ENV:\n",
        "#   env = gym.make('ML4MCSurvival-v0')\n",
        "# else:\n",
        "#   env = gym.make('MineRLObtainDiamond-v0')\n",
        "\n",
        "# for file in os.listdir(directory):\n",
        "#   filename = os.fsdecode(file)\n",
        "#   if filename.endswith(\".pth\"):\n",
        "#     #Setup\n",
        "#     loadName = DIRECTORY_PATH + \"/\" + filename\n",
        "#     network.load_state_dict(th.load(loadName, map_location=th.device('cpu')))\n",
        "\n",
        "#     #Make a new directory for this test\n",
        "#     savePath = SAVE_LOCATION + \"/\" + filename.removesuffix('.pth')\n",
        "#     if not path.exists(savePath):\n",
        "#       os.mkdir(savePath)\n",
        "\n",
        "#     #Finish setting up the recorder and the environment\n",
        "#     env1 = Recorder(env, savePath, fps=60)\n",
        "#     modEnv = ActionShaping(env1, always_attack=True)\n",
        "\n",
        "#     num_actions = modEnv.action_space.n\n",
        "#     action_list = np.arange(num_actions)\n",
        "\n",
        "#     #Print the filename\n",
        "#     print(f\"============{filename}============\")\n",
        "\n",
        "#     data={'FileName':[],\n",
        "#           'Episode':[],\n",
        "#           'Reward':[],\n",
        "#           'Steps':[],\n",
        "#           'y lse 16':[],\n",
        "#           'y between 12 and 5':[]}\n",
        "\n",
        "#     #Running the test\n",
        "#     for episode in range(TEST_EPISODES):\n",
        "#       obs = modEnv.reset()\n",
        "#       done = False\n",
        "#       total_reward = 0\n",
        "#       steps = 0\n",
        "\n",
        "#       depth_reward1 = 0\n",
        "#       depth_reward2 = 0\n",
        "\n",
        "#       # BC part to get some logs:\n",
        "#       for i in tqdm(range(TEST_STEPS)):\n",
        "#           # Process the action:\n",
        "#           #   - Add/remove batch dimensions\n",
        "#           #   - Transpose image (needs to be channels-last)\n",
        "#           #   - Normalize image\n",
        "#           obs = th.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255)\n",
        "#           # Turn logits into probabilities\n",
        "#           probabilities = th.softmax(network(obs), dim=1)[0]\n",
        "#           # Into numpy\n",
        "#           probabilities = probabilities.detach().cpu().numpy()\n",
        "#           # Sample action according to the probabilities\n",
        "#           action = np.random.choice(action_list, p=probabilities)\n",
        "\n",
        "#           obs, reward, done, info = modEnv.step(action)\n",
        "#           # print(\"\\n\\n\\n\\n================OBS===========================\")\n",
        "#           # print(obs)\n",
        "\n",
        "#           if (obs['location_stats']['ypos'] <= 16):\n",
        "#             depth_reward1 += 1\n",
        "#           if (obs['location_stats']['ypos'] <= 12 and obs['location_stats']['ypos'] >= 5):\n",
        "#             depth_reward2 += 1\n",
        "\n",
        "#           if obs['life_stats']['life'] != 20.0:\n",
        "#             print(obs['life_stats']['life'])\n",
        "\n",
        "#           total_reward += reward\n",
        "#           steps += 1\n",
        "#           if done:\n",
        "#               break\n",
        "\n",
        "#       env1.release()\n",
        "#       print(f'FileName: {filename} Episode #{episode + 1} reward: {total_reward}\\t\\t episode length: {steps}\\n')\n",
        "#       print(f'Steps in Depth <=16: {depth_reward1}')\n",
        "#       print(f'Steps in 5 <= Depth <= 12: {depth_reward2}')\n",
        "\n",
        "#       data['FileName'].append(filename)\n",
        "#       data['Episode'].append(episode+1)\n",
        "#       data['Reward'].append(total_reward)\n",
        "#       data['Steps'].append(steps)\n",
        "#       data['y between 12 and 5'].append(depth_reward2)\n",
        "#       data['y lse 16'].append(depth_reward1)\n",
        "\n",
        "#       if FORCE_STOP:\n",
        "#         break\n",
        "\n",
        "#     df = pd.DataFrame(data)\n",
        "#     df.to_csv(f'{savePath}/{filename}_{datetime.now()}.csv', index=False)\n",
        "\n",
        "#     if FORCE_STOP:\n",
        "#       break\n",
        "\n",
        "#     env1.close()\n"
      ],
      "metadata": {
        "id": "NqCI3xDFH-kB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "71399aaa-1589-48e6-9f5c-066b5d356c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "[enforce fail at inline_container.cc:135] . file in archive is not in a subdirectory: data",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e03069669e75>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloadName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDIRECTORY_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Make a new directory for this test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:135] . file in archive is not in a subdirectory: data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "nMly7XWiPRH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOrVEwicwQAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}