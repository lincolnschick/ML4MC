{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "eDMQY4kQ-vxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables to change based upon specifics of test\n",
        "TEST_EPISODES = 1 # Number of tests to run for each model\n",
        "TEST_STEPS = 10000 # Total timesteps to run for each model\n",
        "USING_CUSTOM_ENV = True #Are we using a custom enviroment\n",
        "DIRECTORY_PATH = \"/content/drive/MyDrive/packages/minerl_saved_models\" #Directory we have the models saved in\n",
        "SAVE_LOCATION = \"/content/drive/MyDrive/packages/minerl_test_outputs\" #Directory we are saving videos to\n",
        "FORCE_STOP = False #Force stops after one test (for code testing purposes)"
      ],
      "metadata": {
        "id": "soJAOH11JLDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installations"
      ],
      "metadata": {
        "id": "FogkrTGbACWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "# Allow colab to access google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0jWXGqATTyUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_MINERL = True\n",
        "\n",
        "if LOCAL_MINERL:\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/minerl\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/minerl\")\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\")\n"
      ],
      "metadata": {
        "id": "NmHH-8bnk6Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt install tigervnc-standalone-server\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip3 install gym==0.13.1\n",
        "if LOCAL_MINERL:\n",
        "  !pip3 install -e /content/drive/MyDrive/packages/minerl\n",
        "else:\n",
        "  !pip3 install minerl==0.4.4 --verbose\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!sudo apt-get install xvfb\n",
        "!pip3 install opencv-python\n",
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "3EflRXDLT0uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Environment Setup"
      ],
      "metadata": {
        "id": "3U6bqwuFAKWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minerl.herobraine.env_specs.simple_embodiment import SimpleEmbodimentEnvSpec\n",
        "from minerl.herobraine.hero.handler import Handler\n",
        "from typing import List\n",
        "import random\n",
        "\n",
        "import minerl.herobraine.hero.handlers as handlers\n",
        "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The intent of this env_spec is to create a survival environment for our agent to be evaluated in.\n",
        "This environment allows us to tailor the observation and action spaces to our agent's and UI's needs.\n",
        "\"\"\"\n",
        "\n",
        "NONE = 'none'\n",
        "OTHER = 'other'\n",
        "\n",
        "MS_PER_STEP = 50\n",
        "\n",
        "ML4MC_SURVIVAL_LENGTH = 1 * 60 * 60 * 20  # 1 hour * 60 minutes * 60 seconds * 20 ticks/steps per second\n",
        "\n",
        "class ML4MCSurvival(SimpleEmbodimentEnvSpec):\n",
        "    # ML4MCSurvival constructor\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if 'name' not in kwargs:\n",
        "            kwargs['name'] = 'ML4MCSurvival-v0' # Add environment name if not added\n",
        "\n",
        "        super().__init__(*args, max_episode_steps=ML4MC_SURVIVAL_LENGTH, **kwargs)\n",
        "\n",
        "    # Allows scripts to observe inventory, equipped item, and current location related stats\n",
        "    def create_observables(self) -> List[Handler]:\n",
        "        return super().create_observables() + [\n",
        "            handlers.FlatInventoryObservation(ALL_ITEMS),\n",
        "            handlers.EquippedItemObservation(items=[\n",
        "                'air', 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe', NONE,\n",
        "                OTHER\n",
        "            ], _default='air', _other=OTHER),\n",
        "            handlers.ObservationFromCurrentLocation(),\n",
        "            handlers.ObservationFromLifeStats(),\n",
        "        ]\n",
        "\n",
        "    # Allows scripts to place blocks, equip items, craft items, and smelt items\n",
        "    def create_actionables(self):\n",
        "        return super().create_actionables() + [\n",
        "            handlers.PlaceBlock([NONE, 'dirt', 'stone', 'cobblestone', 'crafting_table', 'furnace', 'torch'],\n",
        "                                _other=NONE, _default=NONE),\n",
        "            handlers.EquipAction([NONE, 'air', 'wooden_axe', 'stone_axe', 'iron_axe', 'stone_sword', 'iron_sword', 'wooden_sword'], _other=NONE, _default=NONE),\n",
        "            handlers.CraftAction([NONE, 'torch', 'stick', 'planks', 'crafting_table'], _other=NONE, _default=NONE),\n",
        "            handlers.CraftNearbyAction(\n",
        "                [NONE, 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe',\n",
        "                 'furnace'], _other=NONE, _default=NONE),\n",
        "            handlers.SmeltItemNearby([NONE, 'iron_ingot', 'coal'], _other=NONE, _default=NONE),\n",
        "        ]\n",
        "\n",
        "    # Rewards for collecting iron (and cobblestone)\n",
        "    def create_rewardables(self) -> List[Handler]:\n",
        "        return [\n",
        "            # handlers.RewardForCollectingItems([\n",
        "            #     dict(type=\"cobblestone\", amount=1, reward=256.0),\n",
        "            #     dict(type=\"dirt\", amount=1, reward=64.0),\n",
        "            # ])\n",
        "            handlers.RewardForXPGain(\n",
        "                reward_per_xp=100.0,\n",
        "                reward_type=\"FIXED\"),\n",
        "            handlers.ConstantReward(constant=1.0)\n",
        "            # handlers.RewardForTakingDMG(\n",
        "            #     reward_per_dmg=10.0,\n",
        "            #     reward_type=\"FIXED\")\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Start the agent with nothing by default, can be modified for testing\n",
        "    def create_agent_start(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.SimpleInventoryAgentStart([\n",
        "                dict(type=\"iron_sword\", quantity=5)\n",
        "            ])\n",
        "        ]\n",
        "\n",
        "    # No agent handlers needed as we are not using any rewards\n",
        "    def create_agent_handlers(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.AgentQuitFromPossessingItem([\n",
        "                dict(type=\"diamond_ore\", amount=32)]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Use the default world generator\n",
        "    def create_server_world_generators(self) -> List[Handler]:\n",
        "        # return [handlers.BiomeGenerator(\"extreme_hills\")]\n",
        "        return [\n",
        "            handlers.FlatWorldGenerator(generatorString=\"3;7,220*1,5*3,2;3;dungeon\")\n",
        "            # handlers.DrawEntityHandler(\n",
        "            #     xpos=str(random.randint(0,100)/10),\n",
        "            #     ypos=str(random.randint(0,100)/10),\n",
        "            #     zpos=str(random.randint(0,100)/10),\n",
        "            #     mobname=\"zombie\"\n",
        "            # )\n",
        "\n",
        "        ]\n",
        "\n",
        "    def create_server_quit_producers(self) -> List[Handler]:\n",
        "        # Set a timeout to end the episode to prevent it from running forever\n",
        "        return [\n",
        "            handlers.ServerQuitFromTimeUp(time_limit_ms=self.max_episode_steps * MS_PER_STEP),\n",
        "            handlers.ServerQuitWhenAnyAgentFinishes()\n",
        "        ]\n",
        "\n",
        "    # This method can be used to change other things about the world such as drawing shapes or spawning a village\n",
        "    # Not needed for ML4MCSurvival\n",
        "    def create_server_decorators(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.DrawingDecorator(\"\"\"\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"16\" y1=\"1\" y2=\"50\" z1=\"-15\" z2=\"16\" />\n",
        "              <DrawCuboid type=\"air\" x1=\"-12\" x2=\"13\" y1=\"4\" y2=\"39\" z1=\"-12\" z2=\"13\" />\n",
        "              <DrawCuboid type=\"glowstone\" x1=\"-14\" x2=\"15\" y1=\"7\" y2=\"30\" z1=\"-14\" z2=\"15\" />\n",
        "            \"\"\")\n",
        "            # handlers.DrawEntityHandler(\n",
        "            #     xpos=str(random.randint(0,100)/10),\n",
        "            #     ypos=str(random.randint(0,100)/10),\n",
        "            #     zpos=str(random.randint(0,100)/10),\n",
        "            #     mobname=\"zombie\"\n",
        "            # )\n",
        "        ]\n",
        "\n",
        "    # This method sets the conditions for the world the agent will spawn into\n",
        "    # We will allow spawning and the passage of time to replicate a realistic Minecraft environment\n",
        "    def create_server_initial_conditions(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.TimeInitialCondition(\n",
        "                start_time=18000, #18000 is night #6000 is day\n",
        "                allow_passage_of_time=True,\n",
        "            ),\n",
        "            handlers.SpawningInitialCondition(\n",
        "                allow_spawning=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def is_from_folder(self, folder: str) -> bool:\n",
        "        return folder == 'ml4mc_survival'\n",
        "\n",
        "    # Don't need docstring as we're not publishing this environment to MineRL's website\n",
        "    def get_docstring(self):\n",
        "        return \"\"\n",
        "\n",
        "    def determine_success_from_rewards(self, rewards: list) -> bool:\n",
        "        # All survival experiemnts are a success =)\n",
        "        return sum(rewards) >= self.reward_threshold"
      ],
      "metadata": {
        "id": "pQ0lkOviQdEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "IGVuu37eAxGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import gym\n",
        "import minerl\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging\n",
        "logging.disable(logging.ERROR)\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "mKJnAaPmTp7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start the Display for saving videos on Colab\n",
        "from pyvirtualdisplay import Display\n",
        "from os import path\n",
        "display = Display(visible=False, size=(400, 300))\n",
        "display.start();"
      ],
      "metadata": {
        "id": "J8ZrxqPoWiZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ],
      "metadata": {
        "id": "p7Gl5a51N0lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionShaping(gym.ActionWrapper):\n",
        "  def __init__(self, env, camera_angle=10, always_attack=False):\n",
        "    super().__init__(env)\n",
        "\n",
        "    self.camera_angle = camera_angle\n",
        "    self.always_attack = always_attack\n",
        "    self._actions = [\n",
        "      [('attack', 1)],\n",
        "      [('forward', 1)],\n",
        "      # [('back', 1)],\n",
        "      # [('left', 1)],\n",
        "      # [('right', 1)],\n",
        "      # [('jump', 1)],\n",
        "      # [('forward', 1), ('attack', 1)],\n",
        "      # [('craft', 'planks')],\n",
        "      [('forward', 1), ('jump', 1)],\n",
        "      [('camera', [-self.camera_angle, 0])],\n",
        "      [('camera', [self.camera_angle, 0])],\n",
        "      [('camera', [0, self.camera_angle])],\n",
        "      [('camera', [0, -self.camera_angle])],\n",
        "    ]\n",
        "\n",
        "    self.actions = []\n",
        "    for actions in self._actions:\n",
        "      act = self.env.action_space.noop()\n",
        "      for a, v in actions:\n",
        "        act[a] = v\n",
        "      if self.always_attack:\n",
        "        act['attack'] = 1\n",
        "      self.actions.append(act)\n",
        "\n",
        "    self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "  def action(self, action):\n",
        "    return self.actions[action]"
      ],
      "metadata": {
        "id": "RRfm-fLaORlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
        "  # There are dummy dimensions of shape one\n",
        "  camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "  attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "  forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "  jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "  batch_size = len(camera_actions)\n",
        "  actions = np.zeros((batch_size,), dtype=np.int)\n",
        "\n",
        "  for i in range(len(camera_actions)):\n",
        "    # Moving camera is most important (horizontal first)\n",
        "    if camera_actions[i][0] < -camera_margin:\n",
        "      actions[i] = 3\n",
        "    elif camera_actions[i][0] > camera_margin:\n",
        "      actions[i] = 4\n",
        "    elif camera_actions[i][1] > camera_margin:\n",
        "      actions[i] = 5\n",
        "    elif camera_actions[i][1] < -camera_margin:\n",
        "      actions[i] = 6\n",
        "    elif forward_actions[i] == 1:\n",
        "      if jump_actions[i] == 1:\n",
        "        actions[i] = 2\n",
        "      else:\n",
        "        actions[i] = 1\n",
        "    elif attack_actions[i] == 1:\n",
        "      actions[i] = 0\n",
        "    else:\n",
        "      # No reasonable mapping (would be no-op)\n",
        "      actions[i] = -1\n",
        "  return actions"
      ],
      "metadata": {
        "id": "ukMZ747COyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_act(env, actions):\n",
        "  act = env.action_space.noop()\n",
        "  for action in actions.split():\n",
        "    if \":\" in action:\n",
        "      k, v = action.split(':')\n",
        "      if k == 'camera':\n",
        "        act[k] = eval(v)\n",
        "      else:\n",
        "        act[k] = v\n",
        "    else:\n",
        "      act[action] = 1\n",
        "  return act"
      ],
      "metadata": {
        "id": "8M2RV4YXPFAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USING_CUSTOM_ENV:\n",
        "  abs_CUSTOM = ML4MCSurvival()\n",
        "  abs_CUSTOM.register()"
      ],
      "metadata": {
        "id": "8Qt-5m-ZNejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install stable-baselines3"
      ],
      "metadata": {
        "id": "wAqdZhEcwJwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "30lI29jkt2xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common import monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "metadata": {
        "id": "YDhtSIzlt2Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "  def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "    super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "    self.check_freq = check_freq\n",
        "    self.log_dir = log_dir\n",
        "    self.save_path = os.path.join(log_dir, 'best_model')\n",
        "    self.best_mean_reward = -np.inf\n",
        "\n",
        "  def _init_callack(self) -> None:\n",
        "    if self.save_path is not None:\n",
        "      os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "    if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "      #Retrieve  Training Reward\n",
        "      x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "      if len(x) > 0:\n",
        "          #Mean training reward over the last 100 episodes\n",
        "          mean_reward = np.mean(y[-100:])\n",
        "          if self.verbose > 0:\n",
        "            print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "            #New best model, save the agent\n",
        "            if mean_reward > self.best_mean_reward:\n",
        "              self.best_mean_reward = mean_reward\n",
        "              #Example for saving best model\n",
        "              if self.verbose > 0:\n",
        "                print(\"Saving new best model to {}\".format(self.save_path))\n",
        "              self.model.save(self.save_path)\n",
        "    return True"
      ],
      "metadata": {
        "id": "sS5wBnEZt6EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# abs_STONE = StoneCollection()\n",
        "# abs_STONE.register() # Register with gym"
      ],
      "metadata": {
        "id": "Zw8n3K0Ft_GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "if USING_CUSTOM_ENV:\n",
        "  env = gym.make('ML4MCSurvival-v0')\n",
        "else:\n",
        "  env = gym.make('MineRLObtainDiamond-v0')"
      ],
      "metadata": {
        "id": "XP5K4SSXuDPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, \"/content/drive/MyDrive/ml4mc_outputs\", fps=60)"
      ],
      "metadata": {
        "id": "WH-fkm76uHJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "id": "igJif-0kuJfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A wrapper for getting the POV of the avatar from the environment, which is needed for stable_baselines\n",
        "class ExtractPOV(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    self.observation_space = self.env.observation_space['pov']\n",
        "\n",
        "  def observation(self, observation):\n",
        "    return observation['pov']"
      ],
      "metadata": {
        "id": "HfpLr5Kgy1fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#callback addition\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "monitored_env = monitor.Monitor(env1, log_dir)\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\"\"\"\n",
        "obs_wrapped_stone = ExtractPOV(env) #Extracting the POV of the avatar from the environment which is needed for stable_baselines\n",
        "obs_action_wrapped_stone = ActionShaping(obs_wrapped_stone) #Performing action shaping on the actions of the environment to convert them from dictionaries into an array.\n",
        "obs = obs_action_wrapped_stone.reset() #reseting the provided environnment\n",
        "\n",
        "\n",
        "\n",
        "model = PPO(policy=\"CnnPolicy\", env=obs_action_wrapped_stone, verbose=1) #Setting the model to be a PPO model with a CnnPolicy. This was just the model used by tutorials, we'll experiment with the best model later\n",
        "model.learn(total_timesteps=50000) #Training the model, allowing it to walk through 50000 timesteps of the environment (about 1.5 minutes)\n",
        "env.release() #releasing the recorded environment to actually make a video on Colab.\n"
      ],
      "metadata": {
        "id": "iaSaJGnduMiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "results_plotter.plot_results([log_dir], 5000, results_plotter.X_TIMESTEPS, \"MineRL RL Training\")\n",
        "plt.show()\n",
        "\"\"\"\n",
        "model.save(DIRECTORY_PATH + \"/\" + 'combat.pth' )"
      ],
      "metadata": {
        "id": "aQeJHpnduOs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOrVEwicwQAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}