{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lincolnschick/ML4MC/blob/main/docs/reports/requirement-48/final_combat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDMQY4kQ-vxw"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soJAOH11JLDB"
      },
      "outputs": [],
      "source": [
        "#Variables to change based upon specifics of test\n",
        "TEST_EPISODES = 1 # Number of tests to run for each model\n",
        "TEST_STEPS = 10000 # Total timesteps to run for each model\n",
        "USING_CUSTOM_ENV = True #Are we using a custom enviroment\n",
        "DIRECTORY_PATH = \"/content/drive/MyDrive/packages/minerl_saved_models\" #Directory we have the models saved in\n",
        "SAVE_LOCATION = \"/content/drive/MyDrive/packages/minerl_test_outputs\" #Directory we are saving videos to\n",
        "FORCE_STOP = False #Force stops after one test (for code testing purposes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FogkrTGbACWe"
      },
      "source": [
        "#Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jWXGqATTyUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0df3e1-cf47-4b59-b88f-43960476aeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "# Allow colab to access google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmHH-8bnk6Gn"
      },
      "outputs": [],
      "source": [
        "LOCAL_MINERL = True\n",
        "\n",
        "if LOCAL_MINERL:\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/minerl\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/minerl\")\n",
        "  !chmod 555 -R \"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\"\n",
        "  sys.path.append(\"/content/drive/MyDrive/packages/MixinGradle-dcfaf61\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EflRXDLT0uC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt install tigervnc-standalone-server\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip3 install gym==0.13.1\n",
        "if LOCAL_MINERL:\n",
        "  !pip3 install -e /content/drive/MyDrive/packages/minerl\n",
        "else:\n",
        "  !pip3 install minerl==0.4.4 --verbose\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender\n",
        "!sudo apt-get install xvfb\n",
        "!pip3 install opencv-python\n",
        "!pip3 install imageio==2.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U6bqwuFAKWH"
      },
      "source": [
        "# Custom Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ0lkOviQdEE"
      },
      "outputs": [],
      "source": [
        "from minerl.herobraine.env_specs.simple_embodiment import SimpleEmbodimentEnvSpec\n",
        "from minerl.herobraine.hero.handler import Handler\n",
        "from typing import List\n",
        "import random\n",
        "\n",
        "import minerl.herobraine.hero.handlers as handlers\n",
        "from minerl.herobraine.hero.mc import ALL_ITEMS\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The intent of this env_spec is to create a survival environment for our agent to be evaluated in.\n",
        "This environment allows us to tailor the observation and action spaces to our agent's and UI's needs.\n",
        "\"\"\"\n",
        "\n",
        "NONE = 'none'\n",
        "OTHER = 'other'\n",
        "\n",
        "MS_PER_STEP = 50\n",
        "\n",
        "ML4MC_SURVIVAL_LENGTH = 1 * 60 * 60 * 20  # 1 hour * 60 minutes * 60 seconds * 20 ticks/steps per second\n",
        "\n",
        "class ML4MCSurvival(SimpleEmbodimentEnvSpec):\n",
        "    # ML4MCSurvival constructor\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if 'name' not in kwargs:\n",
        "            kwargs['name'] = 'ML4MCSurvival-v0' # Add environment name if not added\n",
        "\n",
        "        super().__init__(*args, max_episode_steps=ML4MC_SURVIVAL_LENGTH, **kwargs)\n",
        "        self.mission_xml = self.create_mission()\n",
        "\n",
        "    # Allows scripts to observe inventory, equipped item, and current location related stats\n",
        "    def create_observables(self) -> List[Handler]:\n",
        "        return super().create_observables() + [\n",
        "            handlers.FlatInventoryObservation(ALL_ITEMS),\n",
        "            handlers.EquippedItemObservation(items=[\n",
        "                'air', 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe', 'diamond_sword', 'diamond_helmet', 'diamond_chestplate', 'diamond_leggings',\n",
        "                'diamond_boots', NONE,\n",
        "                OTHER\n",
        "            ], _default='air', _other=OTHER),\n",
        "            handlers.ObservationFromCurrentLocation(),\n",
        "            handlers.ObservationFromLifeStats(),\n",
        "        ]\n",
        "\n",
        "    # Allows scripts to place blocks, equip items, craft items, and smelt items\n",
        "    def create_actionables(self):\n",
        "        return super().create_actionables() + [\n",
        "            handlers.PlaceBlock([NONE, 'dirt', 'stone', 'cobblestone', 'crafting_table', 'furnace', 'torch'],\n",
        "                                _other=NONE, _default=NONE),\n",
        "            handlers.EquipAction([NONE, 'air', 'wooden_axe', 'stone_axe', 'iron_axe', 'stone_sword', 'iron_sword', 'wooden_sword', 'diamond_sword',\n",
        "                                  'diamond_helmet', 'diamond_chestplate', 'diamond_leggings', 'diamond_boots',], _other=NONE, _default=NONE),\n",
        "            handlers.CraftAction([NONE, 'torch', 'stick', 'planks', 'crafting_table'], _other=NONE, _default=NONE),\n",
        "            handlers.CraftNearbyAction(\n",
        "                [NONE, 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe',\n",
        "                 'furnace'], _other=NONE, _default=NONE),\n",
        "            handlers.SmeltItemNearby([NONE, 'iron_ingot', 'coal'], _other=NONE, _default=NONE),\n",
        "        ]\n",
        "\n",
        "    # Rewards for collecting iron (and cobblestone)\n",
        "    def create_rewardables(self) -> List[Handler]:\n",
        "        return [\n",
        "            # XP Reward\n",
        "            handlers.RewardForXPGain(\n",
        "                reward_per_xp=1000.0,\n",
        "                reward_type=\"FIXED\"),\n",
        "            # Damage Negative Reward\n",
        "            handlers.RewardForTakingDMG(\n",
        "                reward_per_dmg=7.0,\n",
        "                reward_type=\"FIXED\"),\n",
        "            # Survival Reward\n",
        "            handlers.RewardForSurviving(\n",
        "                reward_per_tick=5.0),\n",
        "            # Not Walking Reward\n",
        "            handlers.RewardForNotWalking(\n",
        "                reward_per_tick=5.0),\n",
        "            handlers.RewardForTouchingBlockType([\n",
        "                {'type':'obsidian', 'behaviour':'onceOnly', 'reward':1},\n",
        "            ]),\n",
        "            handlers.RewardForCollectingItems([\n",
        "                dict(type=\"diamond_sword\", amount=1, reward=2.0),\n",
        "                dict(type=\"rotten_flesh\", amount=1, reward=50.0),\n",
        "            ])\n",
        "\n",
        "        ]\n",
        "\n",
        "    def create_mission(self):\n",
        "      return f'''\n",
        "      <Mission>\n",
        "          <!-- Other mission parameters -->\n",
        "\n",
        "          <RewardForDamagingEntity>\n",
        "              <Mob type=\"Zombie\" reward=\"40\"/>\n",
        "          </RewardForDamagingEntity>\n",
        "\n",
        "          <AgentHandlers>\n",
        "              <!-- Your agent handlers -->\n",
        "          </AgentHandlers>\n",
        "      </Mission>\n",
        "      '''\n",
        "\n",
        "\n",
        "\n",
        "    # Start the agent with nothing by default, can be modified for testing\n",
        "    def create_agent_start(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.SimpleInventoryAgentStart([\n",
        "                dict(type=\"diamond_sword\", quantity=5),\n",
        "                dict(type=\"diamond_helmet\", quantity=1),\n",
        "                dict(type=\"diamond_chestplate\", quantity=1),\n",
        "                dict(type=\"diamond_leggings\", quantity=1),\n",
        "                dict(type=\"diamond_boots\", quantity=1)\n",
        "            ]),\n",
        "            handlers.AgentStartPlacement(0, 11, 0, 0, 0)\n",
        "        ]\n",
        "\n",
        "    # No agent handlers needed as we are not using any rewards\n",
        "    def create_agent_handlers(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.AgentQuitFromPossessingItem([\n",
        "                dict(type=\"diamond_ore\", amount=32)]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Use the default world generator\n",
        "    def create_server_world_generators(self) -> List[Handler]:\n",
        "        # return [handlers.BiomeGenerator(\"extreme_hills\")]\n",
        "        return [\n",
        "            handlers.FlatWorldGenerator(generatorString=\"1;7,2x3,2;1\"),\n",
        "            handlers.DrawingDecorator(\"\"\"\n",
        "              <DrawCuboid x1=\"-5\" y1=\"7\" z1=\"-5\" x2=\"5\" y2=\"7\" z2=\"5\" type=\"obsidian\"/>\n",
        "              <DrawCuboid x1=\"-6\" y1=\"8\" z1=\"-6\" x2=\"6\" y2=\"9\" z2=\"6\" type=\"fence\"/>\n",
        "              <DrawCuboid x1=\"-5\" y1=\"8\" z1=\"-5\" x2=\"5\" y2=\"9\" z2=\"5\" type=\"air\"/>\n",
        "              <DrawBlock x=\"0\" y=\"8\" z=\"3\" type=\"fence_gate\"/>\n",
        "              <DrawBlock x=\"0\" y=\"9\" z=\"3\" type=\"fence_gate\"/>\n",
        "              <DrawCuboid x1=\"-4\" y1=\"8\" z1=\"1\" x2=\"6\" y2=\"8\" z2=\"1\" type=\"fence\"/>\n",
        "              <DrawEntity x=\"0\" y=\"10\" z=\"2\" type=\"Zombie\"/>\n",
        "          \"\"\")\n",
        "\n",
        "        ]\n",
        "\n",
        "    def create_server_quit_producers(self) -> List[Handler]:\n",
        "        # Set a timeout to end the episode to prevent it from running forever\n",
        "        return [\n",
        "            handlers.ServerQuitFromTimeUp(time_limit_ms=self.max_episode_steps * MS_PER_STEP),\n",
        "            handlers.ServerQuitWhenAnyAgentFinishes()\n",
        "        ]\n",
        "\n",
        "    # This method can be used to change other things about the world such as drawing shapes or spawning a village\n",
        "    # Not needed for ML4MCSurvival\n",
        "    def create_server_decorators(self) -> List[Handler]:\n",
        "        return [\n",
        "            # Attempt at creating an enclosed space\n",
        "            # handlers.DrawingDecorator(\"\"\"\n",
        "            #   <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"16\" y1=\"1\" y2=\"50\" z1=\"-15\" z2=\"16\" />\n",
        "            #   <DrawCuboid type=\"air\" x1=\"-12\" x2=\"13\" y1=\"4\" y2=\"39\" z1=\"-12\" z2=\"13\" />\n",
        "            #   <DrawCuboid type=\"glowstone\" x1=\"-14\" x2=\"15\" y1=\"7\" y2=\"30\" z1=\"-14\" z2=\"15\" />\n",
        "            # \"\"\")\n",
        "\n",
        "            handlers.DrawingDecorator(\"\"\"\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"14\" y1=\"-10\" y2=\"50\" z1=\"16\" z2=\"16\" />\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"16\" y1=\"-10\" y2=\"50\" z1=\"-15\" z2=\"-15\" />\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"-15\" x2=\"-15\" y1=\"-10\" y2=\"50\" z1=\"-15\" z2=\"16\" />\n",
        "              <DrawCuboid type=\"bedrock\" x1=\"16\" x2=\"16\" y1=\"-10\" y2=\"50\" z1=\"-15\" z2=\"16\" />\n",
        "\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "    # This method sets the conditions for the world the agent will spawn into\n",
        "    # We will allow spawning and the passage of time to replicate a realistic Minecraft environment\n",
        "    def create_server_initial_conditions(self) -> List[Handler]:\n",
        "        return [\n",
        "            handlers.TimeInitialCondition(\n",
        "                start_time=15000, #18000 is night #6000 is day\n",
        "                allow_passage_of_time=True,\n",
        "            ),\n",
        "            handlers.SpawningInitialCondition(\n",
        "                allow_spawning=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def is_from_folder(self, folder: str) -> bool:\n",
        "        return folder == 'ml4mc_survival'\n",
        "\n",
        "    # Don't need docstring as we're not publishing this environment to MineRL's website\n",
        "    def get_docstring(self):\n",
        "        return \"\"\n",
        "\n",
        "    def determine_success_from_rewards(self, rewards: list) -> bool:\n",
        "        # All survival experiemnts are a success =)\n",
        "        return sum(rewards) >= self.reward_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGVuu37eAxGR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKJnAaPmTp7z"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import gym\n",
        "import minerl\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging\n",
        "logging.disable(logging.ERROR)\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ZrxqPoWiZi"
      },
      "outputs": [],
      "source": [
        "#Start the Display for saving videos on Colab\n",
        "from pyvirtualdisplay import Display\n",
        "from os import path\n",
        "display = Display(visible=False, size=(400, 300))\n",
        "display.start();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Gl5a51N0lF"
      },
      "outputs": [],
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRfm-fLaORlT"
      },
      "outputs": [],
      "source": [
        "class ActionShaping(gym.ActionWrapper):\n",
        "  def __init__(self, env, camera_angle=10, always_attack=False):\n",
        "    super().__init__(env)\n",
        "\n",
        "    self.camera_angle = camera_angle\n",
        "    self.always_attack = always_attack\n",
        "    self._actions = [\n",
        "      [('attack', 1)],\n",
        "      [('forward', 1)],\n",
        "      # [('back', 1)],\n",
        "      # [('left', 1)],\n",
        "      # [('right', 1)],\n",
        "      # [('jump', 1)],\n",
        "      # [('forward', 1), ('attack', 1)],\n",
        "      # [('craft', 'planks')],\n",
        "      [('forward', 1), ('jump', 1)],\n",
        "      [('camera', [-self.camera_angle, 0])],\n",
        "      [('camera', [self.camera_angle, 0])],\n",
        "      [('camera', [0, self.camera_angle])],\n",
        "      [('camera', [0, -self.camera_angle])],\n",
        "    ]\n",
        "\n",
        "    self.actions = []\n",
        "    for actions in self._actions:\n",
        "      act = self.env.action_space.noop()\n",
        "      for a, v in actions:\n",
        "        act[a] = v\n",
        "      if self.always_attack:\n",
        "        act['attack'] = 1\n",
        "      self.actions.append(act)\n",
        "\n",
        "    self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "  def action(self, action):\n",
        "    return self.actions[action]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukMZ747COyoX"
      },
      "outputs": [],
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
        "  # There are dummy dimensions of shape one\n",
        "  camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "  attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "  forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "  jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "  batch_size = len(camera_actions)\n",
        "  actions = np.zeros((batch_size,), dtype=np.int)\n",
        "\n",
        "  for i in range(len(camera_actions)):\n",
        "    # Moving camera is most important (horizontal first)\n",
        "    if camera_actions[i][0] < -camera_margin:\n",
        "      actions[i] = 3\n",
        "    elif camera_actions[i][0] > camera_margin:\n",
        "      actions[i] = 4\n",
        "    elif camera_actions[i][1] > camera_margin:\n",
        "      actions[i] = 5\n",
        "    elif camera_actions[i][1] < -camera_margin:\n",
        "      actions[i] = 6\n",
        "    elif forward_actions[i] == 1:\n",
        "      if jump_actions[i] == 1:\n",
        "        actions[i] = 2\n",
        "      else:\n",
        "        actions[i] = 1\n",
        "    elif attack_actions[i] == 1:\n",
        "      actions[i] = 0\n",
        "    else:\n",
        "      # No reasonable mapping (would be no-op)\n",
        "      actions[i] = -1\n",
        "  return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M2RV4YXPFAo"
      },
      "outputs": [],
      "source": [
        "def str_to_act(env, actions):\n",
        "  act = env.action_space.noop()\n",
        "  for action in actions.split():\n",
        "    if \":\" in action:\n",
        "      k, v = action.split(':')\n",
        "      if k == 'camera':\n",
        "        act[k] = eval(v)\n",
        "      else:\n",
        "        act[k] = v\n",
        "    else:\n",
        "      act[action] = 1\n",
        "  return act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qt-5m-ZNejQ"
      },
      "outputs": [],
      "source": [
        "abs_CUSTOM = ML4MCSurvival()\n",
        "abs_CUSTOM.register()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAqdZhEcwJwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70fccfb-a4b7-4ef7-9ba2-f6e00a967b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 stable-baselines3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30lI29jkt2xh"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDhtSIzlt2Q7"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common import monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP5K4SSXuDPv"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make('ML4MCSurvival-v0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH-fkm76uHJi"
      },
      "outputs": [],
      "source": [
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, \"/content/drive/MyDrive/ml4mc_outputs/sunday\", fps=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igJif-0kuJfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf46569-cc9f-465d-f5e0-c8e24f677d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install shimmy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfpLr5Kgy1fa"
      },
      "outputs": [],
      "source": [
        "#A wrapper for getting the POV of the avatar from the environment, which is needed for stable_baselines\n",
        "class ExtractPOV(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    self.observation_space = self.env.observation_space['pov']\n",
        "\n",
        "  def observation(self, observation):\n",
        "    return observation['pov']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXOvjiaYTpq1"
      },
      "outputs": [],
      "source": [
        "# y = int(obs['location_stats']['ypos'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaSaJGnduMiw",
        "outputId": "0a7ecc41-070e-444b-ee3a-ef78315945d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 674      |\n",
            "|    ep_rew_mean     | 1        |\n",
            "| time/              |          |\n",
            "|    fps             | 8        |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 241      |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 9           |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 412         |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015719822 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | -0.00175    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.82        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | 0.000531    |\n",
            "|    value_loss           | 31.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 10          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 565         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021549925 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.9        |\n",
            "|    explained_variance   | -0.666      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.54        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0148     |\n",
            "|    value_loss           | 4.49        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 11          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 715         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013602817 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.89       |\n",
            "|    explained_variance   | -0.734      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0809      |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    value_loss           | 0.553       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 11          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 861         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015210921 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | 0.408       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00477    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0252     |\n",
            "|    value_loss           | 0.139       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 1008        |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018099912 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.81       |\n",
            "|    explained_variance   | 0.328       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0493     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0378     |\n",
            "|    value_loss           | 0.0516      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 1148        |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022415059 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.428       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0759     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.049      |\n",
            "|    value_loss           | 0.0331      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 1285        |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033434343 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.593       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0937     |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0643     |\n",
            "|    value_loss           | 0.0196      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 1425        |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049569316 |\n",
            "|    clip_fraction        | 0.374       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.79       |\n",
            "|    explained_variance   | 0.592       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.102      |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.047      |\n",
            "|    value_loss           | 0.0119      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 1560        |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034448802 |\n",
            "|    clip_fraction        | 0.36        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | -0.403      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0861     |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0519     |\n",
            "|    value_loss           | 0.00892     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 674        |\n",
            "|    ep_rew_mean          | 1          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 13         |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 1702       |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02822128 |\n",
            "|    clip_fraction        | 0.334      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.74      |\n",
            "|    explained_variance   | -5.29      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.079     |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0359    |\n",
            "|    value_loss           | 0.00545    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 1837        |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049880676 |\n",
            "|    clip_fraction        | 0.355       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -4.52       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0698     |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0428     |\n",
            "|    value_loss           | 0.00408     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 1969        |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024399718 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.67       |\n",
            "|    explained_variance   | 0.914       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0641     |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0232     |\n",
            "|    value_loss           | 0.00567     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 2100        |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032305382 |\n",
            "|    clip_fraction        | 0.292       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | -0.39       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0974     |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0664     |\n",
            "|    value_loss           | 0.0037      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 2231        |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044724103 |\n",
            "|    clip_fraction        | 0.353       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | -0.352      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.114      |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.071      |\n",
            "|    value_loss           | 0.00277     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 2362        |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042636022 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | 0.456       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0828     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0591     |\n",
            "|    value_loss           | 0.00184     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 674        |\n",
            "|    ep_rew_mean          | 1          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 13         |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 2493       |\n",
            "|    total_timesteps      | 34816      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07430065 |\n",
            "|    clip_fraction        | 0.505      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.72      |\n",
            "|    explained_variance   | -4.94      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.124     |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.0903    |\n",
            "|    value_loss           | 0.00197    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 7.32e+03   |\n",
            "|    ep_rew_mean          | 21         |\n",
            "| time/                   |            |\n",
            "|    fps                  | 13         |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 2706       |\n",
            "|    total_timesteps      | 36864      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04610447 |\n",
            "|    clip_fraction        | 0.247      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.43      |\n",
            "|    explained_variance   | 0.404      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0578    |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.0488    |\n",
            "|    value_loss           | 0.00194    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 4.81e+03   |\n",
            "|    ep_rew_mean          | 13.5       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 13         |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 2913       |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09101468 |\n",
            "|    clip_fraction        | 0.5        |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.51      |\n",
            "|    explained_variance   | -0.533     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0789    |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0738    |\n",
            "|    value_loss           | 0.00497    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.41e+03   |\n",
            "|    ep_rew_mean          | 9.33       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 13         |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 3145       |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09357366 |\n",
            "|    clip_fraction        | 0.526      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.43      |\n",
            "|    explained_variance   | 0.216      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0927    |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0869    |\n",
            "|    value_loss           | 0.00309    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 2.86e+03   |\n",
            "|    ep_rew_mean          | 7.67       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 21         |\n",
            "|    time_elapsed         | 3351       |\n",
            "|    total_timesteps      | 43008      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.10199366 |\n",
            "|    clip_fraction        | 0.521      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.42      |\n",
            "|    explained_variance   | 0.573      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.132     |\n",
            "|    n_updates            | 200        |\n",
            "|    policy_gradient_loss | -0.0865    |\n",
            "|    value_loss           | 0.00239    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.5e+03     |\n",
            "|    ep_rew_mean          | 6.56        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 3557        |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.104562365 |\n",
            "|    clip_fraction        | 0.539       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.661       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.122      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0884     |\n",
            "|    value_loss           | 0.00156     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 2.23e+03   |\n",
            "|    ep_rew_mean          | 5.76       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 3763       |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09862899 |\n",
            "|    clip_fraction        | 0.536      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.51      |\n",
            "|    explained_variance   | 0.65       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.137     |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.0841    |\n",
            "|    value_loss           | 0.00147    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.02e+03    |\n",
            "|    ep_rew_mean          | 5.17        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 12          |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 3967        |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.118706524 |\n",
            "|    clip_fraction        | 0.547       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.676       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.139      |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0913     |\n",
            "|    value_loss           | 0.00158     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.88e+03   |\n",
            "|    ep_rew_mean          | 4.7        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 4174       |\n",
            "|    total_timesteps      | 51200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11933906 |\n",
            "|    clip_fraction        | 0.56       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.52      |\n",
            "|    explained_variance   | 0.784      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.125     |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.0946    |\n",
            "|    value_loss           | 0.000985   |\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "#callback addition\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "monitored_env = monitor.Monitor(env1, log_dir)\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\"\"\"\n",
        "obs_wrapped_combat = ExtractPOV(env) #Extracting the POV of the avatar from the environment which is needed for stable_baselines\n",
        "obs_action_wrapped_combat = ActionShaping(obs_wrapped_combat) #Performing action shaping on the actions of the environment to convert them from dictionaries into an array.\n",
        "obs = obs_action_wrapped_combat.reset() #reseting the provided environnment\n",
        "\n",
        "model = PPO(policy=\"CnnPolicy\", env=obs_action_wrapped_combat, verbose=1) #Setting the model to be a PPO model with a CnnPolicy. This was just the model used by tutorials, we'll experiment with the best model later\n",
        "model.learn(total_timesteps=50000) #Training the model, allowing it to walk through 50000 timesteps of the environment (about 1.5 minutes)\n",
        "# model.learn(total_timesteps=100000000)\n",
        "env.release() #releasing the recorded environment to actually make a video on Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQeJHpnduOs9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "results_plotter.plot_results([log_dir], 5000, results_plotter.X_TIMESTEPS, \"MineRL RL Training\")\n",
        "plt.show()\n",
        "\"\"\"\n",
        "model.save(DIRECTORY_PATH + \"/\" + 'combat.pth' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOrVEwicwQAE"
      },
      "outputs": [],
      "source": [
        "model.save(DIRECTORY_PATH + \"/\" + 'combat2.pth' )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}