{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52e8d5739d0b4855803c40b8f5ddb620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4588138d656420187e5260d4380d4db",
              "IPY_MODEL_87ac8cf3eb964b8e8e30d506569e0a8c",
              "IPY_MODEL_15b8efa1cdc741d0ad5d57e64cfb5fa7"
            ],
            "layout": "IPY_MODEL_10430671c74e4e908f93ed3b28a836e6"
          }
        },
        "c4588138d656420187e5260d4380d4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328222db4df3490f8e5261ec1434b2a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5adf59c57bd140ee94bf2a54fa65d9ad",
            "value": ""
          }
        },
        "87ac8cf3eb964b8e8e30d506569e0a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8371fbc580484c5d921e5521818e8451",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac940af71b4140779bb26b2b9b6ee4ea",
            "value": 1
          }
        },
        "15b8efa1cdc741d0ad5d57e64cfb5fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b92275cbf084842bf62e571ecf8c053",
            "placeholder": "​",
            "style": "IPY_MODEL_273cf8ee82e44563b27090d0583ed971",
            "value": " 1029747/? [3:10:12&lt;00:00, 115.14it/s]"
          }
        },
        "10430671c74e4e908f93ed3b28a836e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328222db4df3490f8e5261ec1434b2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5adf59c57bd140ee94bf2a54fa65d9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8371fbc580484c5d921e5521818e8451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ac940af71b4140779bb26b2b9b6ee4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b92275cbf084842bf62e571ecf8c053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273cf8ee82e44563b27090d0583ed971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lincolnschick/ML4MC/blob/main/docs/reports/requirement-23/Stone_BC_Iron_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSTXmT3YUeF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the folder holding all the data\n",
        "DATASET_PATH = \"/content/drive/MyDrive/packages/data/MineRLObtainIronPickaxe-v0\"\n",
        "\n",
        "# Custom script to modify the rewards to encourage stone collection\n",
        "def update_rewards(filename):\n",
        "  \"\"\"\"\n",
        "  Function that replaces the rewards in the given numpy file\n",
        "  to 0 by default or 1 when the amount of cobblestone in the player's\n",
        "  inventory increases\n",
        "  \"\"\"\n",
        "  data = dict(np.load(filename)) # Load numpy arrays as dictionary for modification\n",
        "  rewards = data[\"reward\"]\n",
        "  stone_inventory = data[\"observation$inventory$cobblestone\"]\n",
        "\n",
        "  last_stone_count = 0 # Initialize stone count in inventory to 0\n",
        "  camera_actions = data[\"action$camera\"]\n",
        "  for i in range(len(stone_inventory) - 1): # The rewards array is smaller than the observation array by 1\n",
        "    rewards[i] = 0 # Remove rewards unrelated to stone collection\n",
        "\n",
        "    if stone_inventory[i] > last_stone_count:\n",
        "      rewards[i] = 1 # Add reward of 1 if we detect the player acquired stone\n",
        "\n",
        "    last_stone_count = stone_inventory[i] # Update stone count\n",
        "\n",
        "  np.savez(filename, **data) # Save each numpy array in the original format\n",
        "\n",
        "# Update rewards for all .npz files in the dataset\n",
        "for path in Path(DATASET_PATH).rglob(\"*.npz\"):\n",
        "  update_rewards(path)\n"
      ],
      "metadata": {
        "id": "wY9_z9SWaCbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "# Allow colab to access google drive\n",
        "drive.mount('/content/drive')\n",
        "# Add minerl's folder to path, so we can install it with pip\n",
        "sys.path.append(\"/content/drive/MyDrive/packages/minerl\")"
      ],
      "metadata": {
        "id": "tLlKGoYOWU9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb\n",
        "!sudo apt-get install xserver-xephyr\n",
        "!sudo apt install tigervnc-standalone-server\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip3 install gym==0.13.1\n",
        "!pip3 install -e /content/drive/MyDrive/packages/minerl\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install -U colabgymrender"
      ],
      "metadata": {
        "id": "_9cQHeTCPSaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmrUKxvYXGa"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8_vZpMFpiD9"
      },
      "source": [
        "import torch as th\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import gym\n",
        "import minerl\n",
        "from minerl.herobraine.env_specs.stone_collection_specs import StoneCollection\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKiasaipYa6l"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyOxGuA5At1g"
      },
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WwX1vgpYfuC"
      },
      "source": [
        "# Environment wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8em_oPbA9PQ"
      },
      "source": [
        "class ActionShaping(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    The default MineRL action space is the following dict:\n",
        "\n",
        "    Dict(attack:Discrete(2),\n",
        "         back:Discrete(2),\n",
        "         camera:Box(low=-180.0, high=180.0, shape=(2,)),\n",
        "         craft:Enum(crafting_table,none,planks,stick,torch),\n",
        "         equip:Enum(air,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "         forward:Discrete(2),\n",
        "         jump:Discrete(2),\n",
        "         left:Discrete(2),\n",
        "         nearbyCraft:Enum(furnace,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "         nearbySmelt:Enum(coal,iron_ingot,none),\n",
        "         place:Enum(cobblestone,crafting_table,dirt,furnace,none,stone,torch),\n",
        "         right:Discrete(2),\n",
        "         sneak:Discrete(2),\n",
        "         sprint:Discrete(2))\n",
        "\n",
        "    It can be viewed as:\n",
        "         - buttons, like attack, back, forward, sprint that are either pressed or not.\n",
        "         - mouse, i.e. the continuous camera action in degrees. The two values are pitch (up/down), where up is\n",
        "           negative, down is positive, and yaw (left/right), where left is negative, right is positive.\n",
        "         - craft/equip/place actions for items specified above.\n",
        "    So an example action could be sprint + forward + jump + attack + turn camera, all in one action.\n",
        "\n",
        "    This wrapper makes the action space much smaller by selecting a few common actions and making the camera actions\n",
        "    discrete. You can change these actions by changing self._actions below. That should just work with the RL agent,\n",
        "    but would require some further tinkering below with the BC one.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, camera_angle=10, always_attack=False):\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.camera_angle = camera_angle\n",
        "        self.always_attack = always_attack\n",
        "        self._actions = [\n",
        "            [('attack', 1)],\n",
        "            [('forward', 1)],\n",
        "            [('forward', 1), ('jump', 1)],\n",
        "            [('camera', [-self.camera_angle, 0])],\n",
        "            [('camera', [self.camera_angle, 0])],\n",
        "            [('camera', [0, self.camera_angle])],\n",
        "            [('camera', [0, -self.camera_angle])],\n",
        "            [('craft', 'planks')],\n",
        "            [('craft', 'stick')],\n",
        "            [('craft', 'crafting_table')],\n",
        "            [('place', 'crafting_table')],\n",
        "            [('nearbyCraft', 'wooden_pickaxe')],\n",
        "            [('nearbyCraft', 'stone_pickaxe')],\n",
        "            [('equip', 'wooden_pickaxe')],\n",
        "            [('equip', 'stone_pickaxe')],\n",
        "        ]\n",
        "\n",
        "        self.actions = []\n",
        "        for actions in self._actions:\n",
        "            act = self.env.action_space.noop()\n",
        "            for a, v in actions:\n",
        "                act[a] = v\n",
        "            if self.always_attack:\n",
        "                act['attack'] = 1\n",
        "            self.actions.append(act)\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "    def action(self, action):\n",
        "        return self.actions[action]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFI364GwY6Oe"
      },
      "source": [
        "# Data parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cZo-d6pA4br"
      },
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
        "    \"\"\"\n",
        "    Turn a batch of actions from dataset (`batch_iter`) to a numpy\n",
        "    array that corresponds to batch of actions of ActionShaping wrapper (_actions).\n",
        "\n",
        "    Camera margin sets the threshold what is considered \"moving camera\".\n",
        "\n",
        "    Note: Hardcoded to work for actions in ActionShaping._actions, with \"intuitive\"\n",
        "        ordering of actions.\n",
        "        If you change ActionShaping._actions, remember to change this!\n",
        "\n",
        "    Array elements are integers corresponding to actions, or \"-1\"\n",
        "    for actions that did not have any corresponding discrete match.\n",
        "    \"\"\"\n",
        "    # There are dummy dimensions of shape one\n",
        "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "    attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "    craft_actions = dataset_actions[\"craft\"].squeeze()\n",
        "    place_actions = dataset_actions[\"place\"].squeeze()\n",
        "    nearby_craft_actions = dataset_actions[\"nearbyCraft\"].squeeze()\n",
        "    equip_actions = dataset_actions[\"equip\"].squeeze()\n",
        "    batch_size = len(camera_actions)\n",
        "    actions = np.zeros((batch_size,), dtype=int)\n",
        "\n",
        "    for i in range(len(camera_actions)):\n",
        "        # Moving camera is most important (horizontal first)\n",
        "        if camera_actions[i][0] < -camera_margin:\n",
        "            actions[i] = 3\n",
        "        elif camera_actions[i][0] > camera_margin:\n",
        "            actions[i] = 4\n",
        "        elif camera_actions[i][1] > camera_margin:\n",
        "            actions[i] = 5\n",
        "        elif camera_actions[i][1] < -camera_margin:\n",
        "            actions[i] = 6\n",
        "        elif forward_actions[i] == 1:\n",
        "            if jump_actions[i] == 1:\n",
        "                actions[i] = 2\n",
        "            else:\n",
        "                actions[i] = 1\n",
        "        elif attack_actions[i] == 1:\n",
        "            actions[i] = 0\n",
        "        elif craft_actions[i] == 'planks':\n",
        "          actions[i] = 7\n",
        "        elif craft_actions[i] == 'stick':\n",
        "          actions[i] = 8\n",
        "        elif craft_actions[i] == 'crafting_table':\n",
        "          actions[i] = 9\n",
        "        elif place_actions[i] == 'crafting_table':\n",
        "          actions[i] = 10\n",
        "        elif nearby_craft_actions[i] == 'wooden_pickaxe':\n",
        "          actions[i] = 11\n",
        "        elif nearby_craft_actions[i] == 'stone_pickaxe':\n",
        "          actions[i] = 12\n",
        "        elif equip_actions[i] == 'wooden_pickaxe':\n",
        "          actions[i] = 13\n",
        "        elif equip_actions[i] == 'stone_pickaxe':\n",
        "          actions[i] = 14\n",
        "        else:\n",
        "            # No reasonable mapping (would be no-op)\n",
        "            actions[i] = -1\n",
        "    return actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg68dO21ZsgG"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5VCVeHyqDlm"
      },
      "source": [
        "# Parameters:\n",
        "TRAIN_MODEL_NAME = 'behavioral_cloning.pth'  # name to use when saving the trained agent.\n",
        "TEST_MODEL_NAME = 'behavioral_cloning.pth'  # name to use when loading the trained agent.\n",
        "\n",
        "TEST_EPISODES = 5  # number of episodes to test the agent for.\n",
        "MAX_TEST_EPISODE_LEN = 5000  # 18k is the default for MineRLObtainDiamond.\n",
        "FINDCAVE_STEPS = 3000  # number of steps to run BC for in evaluations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvrJks0gZCTW"
      },
      "source": [
        "# Setup training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpH8vzpLBGRY"
      },
      "source": [
        "def train(epochs, learning_rate, batch_size):\n",
        "    \"\"\"\n",
        "    :param epochs: How many times we train over the dataset\n",
        "    :param learning_rate: Learning rate for the neural network\n",
        "    :param batch_size: How many samples before the model is updated\n",
        "    \"\"\"\n",
        "\n",
        "    # abs_STONE = StoneCollection()\n",
        "    # abs_STONE.register() # Register with gym\n",
        "    data = minerl.data.make(\"MineRLObtainIronPickaxe-v0\",  data_dir='drive/MyDrive/packages/data', num_workers=4)\n",
        "\n",
        "    # We know ActionShaping has fifteen discrete actions, so we create\n",
        "    # a network to map images to fifteen values (logits), which represent\n",
        "    # likelihoods of selecting those actions\n",
        "    network = NatureCNN((3, 64, 64), 15).cuda()\n",
        "    optimizer = th.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    iter_count = 0\n",
        "    losses = []\n",
        "    for dataset_obs, dataset_actions, _, _, _ in tqdm(data.batch_iter(num_epochs=epochs, batch_size=batch_size, seq_len=1)):\n",
        "        # We only use pov observations (also remove dummy dimensions)\n",
        "        obs = dataset_obs[\"pov\"].squeeze().astype(np.float32)\n",
        "        # Transpose observations to be channel-first (BCHW instead of BHWC)\n",
        "        obs = obs.transpose(0, 3, 1, 2)\n",
        "        # Normalize observations\n",
        "        obs /= 255.0\n",
        "\n",
        "        # Actions need bit more work\n",
        "        actions = dataset_action_batch_to_actions(dataset_actions)\n",
        "\n",
        "        # Remove samples that had no corresponding action\n",
        "        mask = actions != -1\n",
        "        obs = obs[mask]\n",
        "        actions = actions[mask]\n",
        "\n",
        "        # Obtain logits of each action\n",
        "        logits = network(th.from_numpy(obs).float().cuda())\n",
        "\n",
        "        # Minimize cross-entropy with target labels.\n",
        "        # We could also compute the probability of demonstration actions and\n",
        "        # maximize them.\n",
        "        loss = loss_function(logits, th.from_numpy(actions).long().cuda())\n",
        "\n",
        "        # Standard PyTorch update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter_count += 1\n",
        "        losses.append(loss.item())\n",
        "        if (iter_count % 1000) == 0:\n",
        "            mean_loss = sum(losses) / len(losses)\n",
        "            tqdm.write(\"Iteration {}. Loss {:<10.3f}\".format(iter_count, mean_loss))\n",
        "            losses.clear()\n",
        "\n",
        "    th.save(network.state_dict(), TRAIN_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lumAopy0cHBM"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzD13IclpD4T"
      },
      "source": [
        "# Download the Iron Pickaxe dataset\n",
        "minerl.data.download(directory='drive/MyDrive/packages/data', environment='MineRLObtainIronPickaxe-v0');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKLHW_JcRBJ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH84zVpiB19e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52e8d5739d0b4855803c40b8f5ddb620",
            "c4588138d656420187e5260d4380d4db",
            "87ac8cf3eb964b8e8e30d506569e0a8c",
            "15b8efa1cdc741d0ad5d57e64cfb5fa7",
            "10430671c74e4e908f93ed3b28a836e6",
            "328222db4df3490f8e5261ec1434b2a8",
            "5adf59c57bd140ee94bf2a54fa65d9ad",
            "8371fbc580484c5d921e5521818e8451",
            "ac940af71b4140779bb26b2b9b6ee4ea",
            "9b92275cbf084842bf62e571ecf8c053",
            "273cf8ee82e44563b27090d0583ed971"
          ]
        },
        "outputId": "e6cb502f-0f53-4a6c-b5f2-39862d10c3a5"
      },
      "source": [
        "# Train the model for the designated epochs, learning rate, and batch size\n",
        "train(15, 0.0001, 21)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52e8d5739d0b4855803c40b8f5ddb620"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000. Loss 1.106     \n",
            "Iteration 2000. Loss 1.074     \n",
            "Iteration 3000. Loss 1.001     \n",
            "Iteration 4000. Loss 0.931     \n",
            "Iteration 5000. Loss 0.977     \n",
            "Iteration 6000. Loss 0.896     \n",
            "Iteration 7000. Loss 1.032     \n",
            "Iteration 8000. Loss 1.025     \n",
            "Iteration 9000. Loss 0.934     \n",
            "Iteration 10000. Loss 1.056     \n",
            "Iteration 11000. Loss 0.829     \n",
            "Iteration 12000. Loss 0.850     \n",
            "Iteration 13000. Loss 0.874     \n",
            "Iteration 14000. Loss 0.926     \n",
            "Iteration 15000. Loss 0.817     \n",
            "Iteration 16000. Loss 0.795     \n",
            "Iteration 17000. Loss 0.849     \n",
            "Iteration 18000. Loss 0.852     \n",
            "Iteration 19000. Loss 0.885     \n",
            "Iteration 20000. Loss 0.884     \n",
            "Iteration 21000. Loss 0.827     \n",
            "Iteration 22000. Loss 0.945     \n",
            "Iteration 23000. Loss 0.677     \n",
            "Iteration 24000. Loss 0.735     \n",
            "Iteration 25000. Loss 0.789     \n",
            "Iteration 26000. Loss 0.846     \n",
            "Iteration 27000. Loss 0.673     \n",
            "Iteration 28000. Loss 0.905     \n",
            "Iteration 29000. Loss 0.905     \n",
            "Iteration 30000. Loss 0.998     \n",
            "Iteration 31000. Loss 0.949     \n",
            "Iteration 32000. Loss 0.809     \n",
            "Iteration 33000. Loss 0.909     \n",
            "Iteration 34000. Loss 0.855     \n",
            "Iteration 35000. Loss 0.829     \n",
            "Iteration 36000. Loss 0.799     \n",
            "Iteration 37000. Loss 0.828     \n",
            "Iteration 38000. Loss 0.806     \n",
            "Iteration 39000. Loss 0.877     \n",
            "Iteration 40000. Loss 0.889     \n",
            "Iteration 41000. Loss 0.903     \n",
            "Iteration 42000. Loss 0.852     \n",
            "Iteration 43000. Loss 0.771     \n",
            "Iteration 44000. Loss 0.831     \n",
            "Iteration 45000. Loss 0.997     \n",
            "Iteration 46000. Loss 0.960     \n",
            "Iteration 47000. Loss 0.860     \n",
            "Iteration 48000. Loss 0.751     \n",
            "Iteration 49000. Loss 0.793     \n",
            "Iteration 50000. Loss 0.710     \n",
            "Iteration 51000. Loss 0.755     \n",
            "Iteration 52000. Loss 0.761     \n",
            "Iteration 53000. Loss 0.844     \n",
            "Iteration 54000. Loss 0.882     \n",
            "Iteration 55000. Loss 0.952     \n",
            "Iteration 56000. Loss 0.817     \n",
            "Iteration 57000. Loss 0.771     \n",
            "Iteration 58000. Loss 0.822     \n",
            "Iteration 59000. Loss 0.850     \n",
            "Iteration 60000. Loss 0.738     \n",
            "Iteration 61000. Loss 0.843     \n",
            "Iteration 62000. Loss 0.792     \n",
            "Iteration 63000. Loss 0.982     \n",
            "Iteration 64000. Loss 0.779     \n",
            "Iteration 65000. Loss 0.775     \n",
            "Iteration 66000. Loss 0.812     \n",
            "Iteration 67000. Loss 0.888     \n",
            "Iteration 68000. Loss 0.907     \n",
            "Iteration 69000. Loss 0.776     \n",
            "Iteration 70000. Loss 0.644     \n",
            "Iteration 71000. Loss 0.725     \n",
            "Iteration 72000. Loss 0.855     \n",
            "Iteration 73000. Loss 0.759     \n",
            "Iteration 74000. Loss 0.807     \n",
            "Iteration 75000. Loss 0.834     \n",
            "Iteration 76000. Loss 0.889     \n",
            "Iteration 77000. Loss 0.824     \n",
            "Iteration 78000. Loss 0.822     \n",
            "Iteration 79000. Loss 0.679     \n",
            "Iteration 80000. Loss 0.813     \n",
            "Iteration 81000. Loss 0.748     \n",
            "Iteration 82000. Loss 0.773     \n",
            "Iteration 83000. Loss 0.826     \n",
            "Iteration 84000. Loss 0.854     \n",
            "Iteration 85000. Loss 0.878     \n",
            "Iteration 86000. Loss 0.797     \n",
            "Iteration 87000. Loss 0.875     \n",
            "Iteration 88000. Loss 0.856     \n",
            "Iteration 89000. Loss 0.750     \n",
            "Iteration 90000. Loss 0.824     \n",
            "Iteration 91000. Loss 0.871     \n",
            "Iteration 92000. Loss 0.766     \n",
            "Iteration 93000. Loss 0.728     \n",
            "Iteration 94000. Loss 0.831     \n",
            "Iteration 95000. Loss 0.862     \n",
            "Iteration 96000. Loss 0.778     \n",
            "Iteration 97000. Loss 0.793     \n",
            "Iteration 98000. Loss 0.888     \n",
            "Iteration 99000. Loss 0.828     \n",
            "Iteration 100000. Loss 0.918     \n",
            "Iteration 101000. Loss 0.920     \n",
            "Iteration 102000. Loss 0.927     \n",
            "Iteration 103000. Loss 0.807     \n",
            "Iteration 104000. Loss 0.922     \n",
            "Iteration 105000. Loss 0.795     \n",
            "Iteration 106000. Loss 0.869     \n",
            "Iteration 107000. Loss 0.820     \n",
            "Iteration 108000. Loss 0.860     \n",
            "Iteration 109000. Loss 0.789     \n",
            "Iteration 110000. Loss 0.801     \n",
            "Iteration 111000. Loss 0.829     \n",
            "Iteration 112000. Loss 0.853     \n",
            "Iteration 113000. Loss 0.963     \n",
            "Iteration 114000. Loss 0.844     \n",
            "Iteration 115000. Loss 0.802     \n",
            "Iteration 116000. Loss 0.835     \n",
            "Iteration 117000. Loss 0.776     \n",
            "Iteration 118000. Loss 0.783     \n",
            "Iteration 119000. Loss 0.831     \n",
            "Iteration 120000. Loss 0.711     \n",
            "Iteration 121000. Loss 0.818     \n",
            "Iteration 122000. Loss 0.782     \n",
            "Iteration 123000. Loss 0.812     \n",
            "Iteration 124000. Loss 0.918     \n",
            "Iteration 125000. Loss 0.826     \n",
            "Iteration 126000. Loss 0.804     \n",
            "Iteration 127000. Loss 0.729     \n",
            "Iteration 128000. Loss 0.787     \n",
            "Iteration 129000. Loss 0.834     \n",
            "Iteration 130000. Loss 0.817     \n",
            "Iteration 131000. Loss 0.776     \n",
            "Iteration 132000. Loss 0.734     \n",
            "Iteration 133000. Loss 0.713     \n",
            "Iteration 134000. Loss 0.778     \n",
            "Iteration 135000. Loss 0.793     \n",
            "Iteration 136000. Loss 0.807     \n",
            "Iteration 137000. Loss 0.850     \n",
            "Iteration 138000. Loss 0.860     \n",
            "Iteration 139000. Loss 0.597     \n",
            "Iteration 140000. Loss 0.768     \n",
            "Iteration 141000. Loss 0.878     \n",
            "Iteration 142000. Loss 0.881     \n",
            "Iteration 143000. Loss 0.820     \n",
            "Iteration 144000. Loss 0.767     \n",
            "Iteration 145000. Loss 0.820     \n",
            "Iteration 146000. Loss 0.872     \n",
            "Iteration 147000. Loss 0.795     \n",
            "Iteration 148000. Loss 0.778     \n",
            "Iteration 149000. Loss 0.786     \n",
            "Iteration 150000. Loss 0.857     \n",
            "Iteration 151000. Loss 0.748     \n",
            "Iteration 152000. Loss 0.813     \n",
            "Iteration 153000. Loss 0.895     \n",
            "Iteration 154000. Loss 0.763     \n",
            "Iteration 155000. Loss 0.777     \n",
            "Iteration 156000. Loss 0.837     \n",
            "Iteration 157000. Loss 0.801     \n",
            "Iteration 158000. Loss 0.902     \n",
            "Iteration 159000. Loss 0.884     \n",
            "Iteration 160000. Loss 0.802     \n",
            "Iteration 161000. Loss 0.850     \n",
            "Iteration 162000. Loss 0.739     \n",
            "Iteration 163000. Loss 0.717     \n",
            "Iteration 164000. Loss 0.799     \n",
            "Iteration 165000. Loss 0.773     \n",
            "Iteration 166000. Loss 0.918     \n",
            "Iteration 167000. Loss 0.719     \n",
            "Iteration 168000. Loss 0.873     \n",
            "Iteration 169000. Loss 0.782     \n",
            "Iteration 170000. Loss 0.709     \n",
            "Iteration 171000. Loss 0.705     \n",
            "Iteration 172000. Loss 0.826     \n",
            "Iteration 173000. Loss 0.883     \n",
            "Iteration 174000. Loss 0.957     \n",
            "Iteration 175000. Loss 0.698     \n",
            "Iteration 176000. Loss 0.830     \n",
            "Iteration 177000. Loss 0.820     \n",
            "Iteration 178000. Loss 0.873     \n",
            "Iteration 179000. Loss 0.848     \n",
            "Iteration 180000. Loss 0.704     \n",
            "Iteration 181000. Loss 0.802     \n",
            "Iteration 182000. Loss 0.856     \n",
            "Iteration 183000. Loss 0.799     \n",
            "Iteration 184000. Loss 0.820     \n",
            "Iteration 185000. Loss 0.758     \n",
            "Iteration 186000. Loss 0.814     \n",
            "Iteration 187000. Loss 0.924     \n",
            "Iteration 188000. Loss 0.781     \n",
            "Iteration 189000. Loss 0.840     \n",
            "Iteration 190000. Loss 0.692     \n",
            "Iteration 191000. Loss 0.698     \n",
            "Iteration 192000. Loss 0.886     \n",
            "Iteration 193000. Loss 0.928     \n",
            "Iteration 194000. Loss 0.747     \n",
            "Iteration 195000. Loss 0.749     \n",
            "Iteration 196000. Loss 0.737     \n",
            "Iteration 197000. Loss 0.843     \n",
            "Iteration 198000. Loss 0.901     \n",
            "Iteration 199000. Loss 0.757     \n",
            "Iteration 200000. Loss 0.828     \n",
            "Iteration 201000. Loss 0.776     \n",
            "Iteration 202000. Loss 0.799     \n",
            "Iteration 203000. Loss 0.861     \n",
            "Iteration 204000. Loss 0.872     \n",
            "Iteration 205000. Loss 0.895     \n",
            "Iteration 206000. Loss 0.841     \n",
            "Iteration 207000. Loss 0.870     \n",
            "Iteration 208000. Loss 0.701     \n",
            "Iteration 209000. Loss 0.733     \n",
            "Iteration 210000. Loss 0.777     \n",
            "Iteration 211000. Loss 0.722     \n",
            "Iteration 212000. Loss 0.763     \n",
            "Iteration 213000. Loss 0.823     \n",
            "Iteration 214000. Loss 0.706     \n",
            "Iteration 215000. Loss 0.753     \n",
            "Iteration 216000. Loss 0.843     \n",
            "Iteration 217000. Loss 0.690     \n",
            "Iteration 218000. Loss 0.805     \n",
            "Iteration 219000. Loss 0.895     \n",
            "Iteration 220000. Loss 0.900     \n",
            "Iteration 221000. Loss 0.918     \n",
            "Iteration 222000. Loss 0.840     \n",
            "Iteration 223000. Loss 0.763     \n",
            "Iteration 224000. Loss 0.757     \n",
            "Iteration 225000. Loss 0.757     \n",
            "Iteration 226000. Loss 0.798     \n",
            "Iteration 227000. Loss 0.830     \n",
            "Iteration 228000. Loss 0.817     \n",
            "Iteration 229000. Loss 0.709     \n",
            "Iteration 230000. Loss 0.781     \n",
            "Iteration 231000. Loss 0.789     \n",
            "Iteration 232000. Loss 0.839     \n",
            "Iteration 233000. Loss 0.826     \n",
            "Iteration 234000. Loss 0.857     \n",
            "Iteration 235000. Loss 0.940     \n",
            "Iteration 236000. Loss 0.813     \n",
            "Iteration 237000. Loss 0.794     \n",
            "Iteration 238000. Loss 0.773     \n",
            "Iteration 239000. Loss 0.987     \n",
            "Iteration 240000. Loss 0.976     \n",
            "Iteration 241000. Loss 0.836     \n",
            "Iteration 242000. Loss 0.832     \n",
            "Iteration 243000. Loss 0.788     \n",
            "Iteration 244000. Loss 0.885     \n",
            "Iteration 245000. Loss 0.923     \n",
            "Iteration 246000. Loss 0.741     \n",
            "Iteration 247000. Loss 0.823     \n",
            "Iteration 248000. Loss 0.642     \n",
            "Iteration 249000. Loss 0.821     \n",
            "Iteration 250000. Loss 0.699     \n",
            "Iteration 251000. Loss 0.845     \n",
            "Iteration 252000. Loss 0.932     \n",
            "Iteration 253000. Loss 0.862     \n",
            "Iteration 254000. Loss 0.726     \n",
            "Iteration 255000. Loss 0.841     \n",
            "Iteration 256000. Loss 0.821     \n",
            "Iteration 257000. Loss 0.921     \n",
            "Iteration 258000. Loss 0.814     \n",
            "Iteration 259000. Loss 0.783     \n",
            "Iteration 260000. Loss 0.934     \n",
            "Iteration 261000. Loss 0.831     \n",
            "Iteration 262000. Loss 0.812     \n",
            "Iteration 263000. Loss 0.834     \n",
            "Iteration 264000. Loss 0.837     \n",
            "Iteration 265000. Loss 0.901     \n",
            "Iteration 266000. Loss 0.815     \n",
            "Iteration 267000. Loss 0.764     \n",
            "Iteration 268000. Loss 0.753     \n",
            "Iteration 269000. Loss 0.725     \n",
            "Iteration 270000. Loss 0.791     \n",
            "Iteration 271000. Loss 0.815     \n",
            "Iteration 272000. Loss 0.821     \n",
            "Iteration 273000. Loss 0.735     \n",
            "Iteration 274000. Loss 0.752     \n",
            "Iteration 275000. Loss 0.798     \n",
            "Iteration 276000. Loss 0.732     \n",
            "Iteration 277000. Loss 0.659     \n",
            "Iteration 278000. Loss 0.797     \n",
            "Iteration 279000. Loss 0.730     \n",
            "Iteration 280000. Loss 0.841     \n",
            "Iteration 281000. Loss 0.930     \n",
            "Iteration 282000. Loss 0.810     \n",
            "Iteration 283000. Loss 0.884     \n",
            "Iteration 284000. Loss 0.721     \n",
            "Iteration 285000. Loss 0.713     \n",
            "Iteration 286000. Loss 0.739     \n",
            "Iteration 287000. Loss 0.726     \n",
            "Iteration 288000. Loss 0.740     \n",
            "Iteration 289000. Loss 0.802     \n",
            "Iteration 290000. Loss 0.779     \n",
            "Iteration 291000. Loss 0.770     \n",
            "Iteration 292000. Loss 0.670     \n",
            "Iteration 293000. Loss 0.815     \n",
            "Iteration 294000. Loss 0.785     \n",
            "Iteration 295000. Loss 0.771     \n",
            "Iteration 296000. Loss 0.769     \n",
            "Iteration 297000. Loss 0.738     \n",
            "Iteration 298000. Loss 0.926     \n",
            "Iteration 299000. Loss 0.805     \n",
            "Iteration 300000. Loss 0.768     \n",
            "Iteration 301000. Loss 0.825     \n",
            "Iteration 302000. Loss 0.828     \n",
            "Iteration 303000. Loss 0.810     \n",
            "Iteration 304000. Loss 0.807     \n",
            "Iteration 305000. Loss 0.675     \n",
            "Iteration 306000. Loss 0.800     \n",
            "Iteration 307000. Loss 0.850     \n",
            "Iteration 308000. Loss 0.954     \n",
            "Iteration 309000. Loss 0.730     \n",
            "Iteration 310000. Loss 0.880     \n",
            "Iteration 311000. Loss 0.791     \n",
            "Iteration 312000. Loss 0.841     \n",
            "Iteration 313000. Loss 0.832     \n",
            "Iteration 314000. Loss 0.866     \n",
            "Iteration 315000. Loss 0.840     \n",
            "Iteration 316000. Loss 0.922     \n",
            "Iteration 317000. Loss 0.791     \n",
            "Iteration 318000. Loss 0.805     \n",
            "Iteration 319000. Loss 0.895     \n",
            "Iteration 320000. Loss 0.727     \n",
            "Iteration 321000. Loss 0.775     \n",
            "Iteration 322000. Loss 0.854     \n",
            "Iteration 323000. Loss 0.954     \n",
            "Iteration 324000. Loss 0.808     \n",
            "Iteration 325000. Loss 0.861     \n",
            "Iteration 326000. Loss 0.833     \n",
            "Iteration 327000. Loss 0.756     \n",
            "Iteration 328000. Loss 0.859     \n",
            "Iteration 329000. Loss 0.764     \n",
            "Iteration 330000. Loss 0.823     \n",
            "Iteration 331000. Loss 0.795     \n",
            "Iteration 332000. Loss 0.726     \n",
            "Iteration 333000. Loss 0.798     \n",
            "Iteration 334000. Loss 0.741     \n",
            "Iteration 335000. Loss 0.738     \n",
            "Iteration 336000. Loss 0.881     \n",
            "Iteration 337000. Loss 0.860     \n",
            "Iteration 338000. Loss 0.826     \n",
            "Iteration 339000. Loss 0.840     \n",
            "Iteration 340000. Loss 0.806     \n",
            "Iteration 341000. Loss 0.769     \n",
            "Iteration 342000. Loss 0.779     \n",
            "Iteration 343000. Loss 0.782     \n",
            "Iteration 344000. Loss 0.717     \n",
            "Iteration 345000. Loss 0.695     \n",
            "Iteration 346000. Loss 0.728     \n",
            "Iteration 347000. Loss 0.767     \n",
            "Iteration 348000. Loss 0.861     \n",
            "Iteration 349000. Loss 0.812     \n",
            "Iteration 350000. Loss 0.926     \n",
            "Iteration 351000. Loss 0.884     \n",
            "Iteration 352000. Loss 0.892     \n",
            "Iteration 353000. Loss 0.893     \n",
            "Iteration 354000. Loss 0.762     \n",
            "Iteration 355000. Loss 0.774     \n",
            "Iteration 356000. Loss 0.800     \n",
            "Iteration 357000. Loss 0.636     \n",
            "Iteration 358000. Loss 0.775     \n",
            "Iteration 359000. Loss 0.754     \n",
            "Iteration 360000. Loss 0.657     \n",
            "Iteration 361000. Loss 0.894     \n",
            "Iteration 362000. Loss 0.795     \n",
            "Iteration 363000. Loss 0.822     \n",
            "Iteration 364000. Loss 0.871     \n",
            "Iteration 365000. Loss 0.907     \n",
            "Iteration 366000. Loss 0.889     \n",
            "Iteration 367000. Loss 0.724     \n",
            "Iteration 368000. Loss 0.717     \n",
            "Iteration 369000. Loss 0.877     \n",
            "Iteration 370000. Loss 0.829     \n",
            "Iteration 371000. Loss 0.904     \n",
            "Iteration 372000. Loss 0.737     \n",
            "Iteration 373000. Loss 0.770     \n",
            "Iteration 374000. Loss 0.759     \n",
            "Iteration 375000. Loss 0.858     \n",
            "Iteration 376000. Loss 0.730     \n",
            "Iteration 377000. Loss 0.768     \n",
            "Iteration 378000. Loss 0.639     \n",
            "Iteration 379000. Loss 0.829     \n",
            "Iteration 380000. Loss 0.932     \n",
            "Iteration 381000. Loss 0.755     \n",
            "Iteration 382000. Loss 0.833     \n",
            "Iteration 383000. Loss 0.925     \n",
            "Iteration 384000. Loss 0.759     \n",
            "Iteration 385000. Loss 0.771     \n",
            "Iteration 386000. Loss 0.887     \n",
            "Iteration 387000. Loss 0.918     \n",
            "Iteration 388000. Loss 0.902     \n",
            "Iteration 389000. Loss 0.807     \n",
            "Iteration 390000. Loss 0.780     \n",
            "Iteration 391000. Loss 0.860     \n",
            "Iteration 392000. Loss 0.892     \n",
            "Iteration 393000. Loss 0.760     \n",
            "Iteration 394000. Loss 0.886     \n",
            "Iteration 395000. Loss 0.802     \n",
            "Iteration 396000. Loss 0.777     \n",
            "Iteration 397000. Loss 0.708     \n",
            "Iteration 398000. Loss 0.831     \n",
            "Iteration 399000. Loss 0.917     \n",
            "Iteration 400000. Loss 0.826     \n",
            "Iteration 401000. Loss 0.713     \n",
            "Iteration 402000. Loss 0.776     \n",
            "Iteration 403000. Loss 0.814     \n",
            "Iteration 404000. Loss 0.911     \n",
            "Iteration 405000. Loss 0.783     \n",
            "Iteration 406000. Loss 0.747     \n",
            "Iteration 407000. Loss 0.835     \n",
            "Iteration 408000. Loss 0.710     \n",
            "Iteration 409000. Loss 0.738     \n",
            "Iteration 410000. Loss 0.724     \n",
            "Iteration 411000. Loss 0.731     \n",
            "Iteration 412000. Loss 0.539     \n",
            "Iteration 413000. Loss 0.915     \n",
            "Iteration 414000. Loss 0.900     \n",
            "Iteration 415000. Loss 0.739     \n",
            "Iteration 416000. Loss 0.851     \n",
            "Iteration 417000. Loss 0.882     \n",
            "Iteration 418000. Loss 0.918     \n",
            "Iteration 419000. Loss 0.860     \n",
            "Iteration 420000. Loss 0.920     \n",
            "Iteration 421000. Loss 0.852     \n",
            "Iteration 422000. Loss 0.855     \n",
            "Iteration 423000. Loss 0.860     \n",
            "Iteration 424000. Loss 0.798     \n",
            "Iteration 425000. Loss 0.811     \n",
            "Iteration 426000. Loss 0.822     \n",
            "Iteration 427000. Loss 0.766     \n",
            "Iteration 428000. Loss 0.781     \n",
            "Iteration 429000. Loss 0.681     \n",
            "Iteration 430000. Loss 0.718     \n",
            "Iteration 431000. Loss 0.691     \n",
            "Iteration 432000. Loss 0.773     \n",
            "Iteration 433000. Loss 0.715     \n",
            "Iteration 434000. Loss 0.654     \n",
            "Iteration 435000. Loss 0.740     \n",
            "Iteration 436000. Loss 0.640     \n",
            "Iteration 437000. Loss 0.743     \n",
            "Iteration 438000. Loss 0.759     \n",
            "Iteration 439000. Loss 0.840     \n",
            "Iteration 440000. Loss 0.772     \n",
            "Iteration 441000. Loss 0.694     \n",
            "Iteration 442000. Loss 0.908     \n",
            "Iteration 443000. Loss 0.959     \n",
            "Iteration 444000. Loss 0.832     \n",
            "Iteration 445000. Loss 0.760     \n",
            "Iteration 446000. Loss 0.801     \n",
            "Iteration 447000. Loss 0.838     \n",
            "Iteration 448000. Loss 0.704     \n",
            "Iteration 449000. Loss 0.748     \n",
            "Iteration 450000. Loss 0.782     \n",
            "Iteration 451000. Loss 0.781     \n",
            "Iteration 452000. Loss 0.820     \n",
            "Iteration 453000. Loss 0.915     \n",
            "Iteration 454000. Loss 0.813     \n",
            "Iteration 455000. Loss 0.832     \n",
            "Iteration 456000. Loss 0.776     \n",
            "Iteration 457000. Loss 0.807     \n",
            "Iteration 458000. Loss 0.810     \n",
            "Iteration 459000. Loss 0.762     \n",
            "Iteration 460000. Loss 0.681     \n",
            "Iteration 461000. Loss 0.776     \n",
            "Iteration 462000. Loss 0.701     \n",
            "Iteration 463000. Loss 0.874     \n",
            "Iteration 464000. Loss 0.881     \n",
            "Iteration 465000. Loss 0.779     \n",
            "Iteration 466000. Loss 0.837     \n",
            "Iteration 467000. Loss 0.866     \n",
            "Iteration 468000. Loss 0.784     \n",
            "Iteration 469000. Loss 0.818     \n",
            "Iteration 470000. Loss 0.903     \n",
            "Iteration 471000. Loss 0.833     \n",
            "Iteration 472000. Loss 0.719     \n",
            "Iteration 473000. Loss 0.876     \n",
            "Iteration 474000. Loss 0.844     \n",
            "Iteration 475000. Loss 0.785     \n",
            "Iteration 476000. Loss 0.726     \n",
            "Iteration 477000. Loss 0.795     \n",
            "Iteration 478000. Loss 0.715     \n",
            "Iteration 479000. Loss 0.855     \n",
            "Iteration 480000. Loss 0.827     \n",
            "Iteration 481000. Loss 0.880     \n",
            "Iteration 482000. Loss 0.717     \n",
            "Iteration 483000. Loss 0.750     \n",
            "Iteration 484000. Loss 0.737     \n",
            "Iteration 485000. Loss 0.786     \n",
            "Iteration 486000. Loss 0.913     \n",
            "Iteration 487000. Loss 0.821     \n",
            "Iteration 488000. Loss 0.792     \n",
            "Iteration 489000. Loss 0.868     \n",
            "Iteration 490000. Loss 0.802     \n",
            "Iteration 491000. Loss 0.718     \n",
            "Iteration 492000. Loss 0.791     \n",
            "Iteration 493000. Loss 0.804     \n",
            "Iteration 494000. Loss 0.790     \n",
            "Iteration 495000. Loss 0.842     \n",
            "Iteration 496000. Loss 0.891     \n",
            "Iteration 497000. Loss 0.808     \n",
            "Iteration 498000. Loss 1.023     \n",
            "Iteration 499000. Loss 0.813     \n",
            "Iteration 500000. Loss 0.823     \n",
            "Iteration 501000. Loss 0.638     \n",
            "Iteration 502000. Loss 0.732     \n",
            "Iteration 503000. Loss 0.745     \n",
            "Iteration 504000. Loss 0.883     \n",
            "Iteration 505000. Loss 0.788     \n",
            "Iteration 506000. Loss 0.700     \n",
            "Iteration 507000. Loss 0.825     \n",
            "Iteration 508000. Loss 0.756     \n",
            "Iteration 509000. Loss 0.832     \n",
            "Iteration 510000. Loss 0.766     \n",
            "Iteration 511000. Loss 0.860     \n",
            "Iteration 512000. Loss 0.818     \n",
            "Iteration 513000. Loss 0.819     \n",
            "Iteration 514000. Loss 0.867     \n",
            "Iteration 515000. Loss 0.792     \n",
            "Iteration 516000. Loss 0.828     \n",
            "Iteration 517000. Loss 0.800     \n",
            "Iteration 518000. Loss 0.769     \n",
            "Iteration 519000. Loss 0.715     \n",
            "Iteration 520000. Loss 0.736     \n",
            "Iteration 521000. Loss 0.776     \n",
            "Iteration 522000. Loss 0.791     \n",
            "Iteration 523000. Loss 0.742     \n",
            "Iteration 524000. Loss 0.921     \n",
            "Iteration 525000. Loss 0.882     \n",
            "Iteration 526000. Loss 0.933     \n",
            "Iteration 527000. Loss 0.868     \n",
            "Iteration 528000. Loss 0.782     \n",
            "Iteration 529000. Loss 0.827     \n",
            "Iteration 530000. Loss 0.905     \n",
            "Iteration 531000. Loss 0.869     \n",
            "Iteration 532000. Loss 0.674     \n",
            "Iteration 533000. Loss 0.853     \n",
            "Iteration 534000. Loss 0.849     \n",
            "Iteration 535000. Loss 0.872     \n",
            "Iteration 536000. Loss 0.841     \n",
            "Iteration 537000. Loss 0.729     \n",
            "Iteration 538000. Loss 0.722     \n",
            "Iteration 539000. Loss 0.723     \n",
            "Iteration 540000. Loss 0.833     \n",
            "Iteration 541000. Loss 0.791     \n",
            "Iteration 542000. Loss 0.680     \n",
            "Iteration 543000. Loss 0.883     \n",
            "Iteration 544000. Loss 0.716     \n",
            "Iteration 545000. Loss 0.890     \n",
            "Iteration 546000. Loss 0.857     \n",
            "Iteration 547000. Loss 0.700     \n",
            "Iteration 548000. Loss 0.679     \n",
            "Iteration 549000. Loss 0.771     \n",
            "Iteration 550000. Loss 0.794     \n",
            "Iteration 551000. Loss 0.649     \n",
            "Iteration 552000. Loss 0.732     \n",
            "Iteration 553000. Loss 0.859     \n",
            "Iteration 554000. Loss 0.758     \n",
            "Iteration 555000. Loss 0.730     \n",
            "Iteration 556000. Loss 0.659     \n",
            "Iteration 557000. Loss 0.798     \n",
            "Iteration 558000. Loss 0.710     \n",
            "Iteration 559000. Loss 0.735     \n",
            "Iteration 560000. Loss 0.818     \n",
            "Iteration 561000. Loss 0.888     \n",
            "Iteration 562000. Loss 0.844     \n",
            "Iteration 563000. Loss 0.780     \n",
            "Iteration 564000. Loss 0.910     \n",
            "Iteration 565000. Loss 0.832     \n",
            "Iteration 566000. Loss 0.813     \n",
            "Iteration 567000. Loss 0.734     \n",
            "Iteration 568000. Loss 0.821     \n",
            "Iteration 569000. Loss 0.720     \n",
            "Iteration 570000. Loss 0.624     \n",
            "Iteration 571000. Loss 0.730     \n",
            "Iteration 572000. Loss 0.813     \n",
            "Iteration 573000. Loss 0.863     \n",
            "Iteration 574000. Loss 0.976     \n",
            "Iteration 575000. Loss 0.739     \n",
            "Iteration 576000. Loss 0.838     \n",
            "Iteration 577000. Loss 0.908     \n",
            "Iteration 578000. Loss 0.878     \n",
            "Iteration 579000. Loss 0.782     \n",
            "Iteration 580000. Loss 0.900     \n",
            "Iteration 581000. Loss 0.972     \n",
            "Iteration 582000. Loss 0.904     \n",
            "Iteration 583000. Loss 0.822     \n",
            "Iteration 584000. Loss 0.764     \n",
            "Iteration 585000. Loss 0.821     \n",
            "Iteration 586000. Loss 0.668     \n",
            "Iteration 587000. Loss 0.818     \n",
            "Iteration 588000. Loss 0.860     \n",
            "Iteration 589000. Loss 0.905     \n",
            "Iteration 590000. Loss 0.914     \n",
            "Iteration 591000. Loss 0.731     \n",
            "Iteration 592000. Loss 0.778     \n",
            "Iteration 593000. Loss 0.765     \n",
            "Iteration 594000. Loss 0.846     \n",
            "Iteration 595000. Loss 0.815     \n",
            "Iteration 596000. Loss 0.825     \n",
            "Iteration 597000. Loss 0.750     \n",
            "Iteration 598000. Loss 0.730     \n",
            "Iteration 599000. Loss 0.873     \n",
            "Iteration 600000. Loss 0.758     \n",
            "Iteration 601000. Loss 0.719     \n",
            "Iteration 602000. Loss 0.892     \n",
            "Iteration 603000. Loss 0.754     \n",
            "Iteration 604000. Loss 0.685     \n",
            "Iteration 605000. Loss 0.664     \n",
            "Iteration 606000. Loss 0.796     \n",
            "Iteration 607000. Loss 0.812     \n",
            "Iteration 608000. Loss 0.847     \n",
            "Iteration 609000. Loss 0.761     \n",
            "Iteration 610000. Loss 0.856     \n",
            "Iteration 611000. Loss 0.763     \n",
            "Iteration 612000. Loss 0.662     \n",
            "Iteration 613000. Loss 0.717     \n",
            "Iteration 614000. Loss 0.820     \n",
            "Iteration 615000. Loss 0.849     \n",
            "Iteration 616000. Loss 0.888     \n",
            "Iteration 617000. Loss 0.985     \n",
            "Iteration 618000. Loss 0.851     \n",
            "Iteration 619000. Loss 0.772     \n",
            "Iteration 620000. Loss 0.597     \n",
            "Iteration 621000. Loss 0.756     \n",
            "Iteration 622000. Loss 0.729     \n",
            "Iteration 623000. Loss 0.740     \n",
            "Iteration 624000. Loss 0.648     \n",
            "Iteration 625000. Loss 0.699     \n",
            "Iteration 626000. Loss 0.903     \n",
            "Iteration 627000. Loss 0.771     \n",
            "Iteration 628000. Loss 0.858     \n",
            "Iteration 629000. Loss 0.951     \n",
            "Iteration 630000. Loss 0.883     \n",
            "Iteration 631000. Loss 0.911     \n",
            "Iteration 632000. Loss 0.809     \n",
            "Iteration 633000. Loss 0.844     \n",
            "Iteration 634000. Loss 0.960     \n",
            "Iteration 635000. Loss 0.703     \n",
            "Iteration 636000. Loss 0.662     \n",
            "Iteration 637000. Loss 0.781     \n",
            "Iteration 638000. Loss 0.762     \n",
            "Iteration 639000. Loss 0.839     \n",
            "Iteration 640000. Loss 0.827     \n",
            "Iteration 641000. Loss 0.670     \n",
            "Iteration 642000. Loss 0.718     \n",
            "Iteration 643000. Loss 0.756     \n",
            "Iteration 644000. Loss 0.715     \n",
            "Iteration 645000. Loss 0.749     \n",
            "Iteration 646000. Loss 0.861     \n",
            "Iteration 647000. Loss 1.006     \n",
            "Iteration 648000. Loss 0.786     \n",
            "Iteration 649000. Loss 0.759     \n",
            "Iteration 650000. Loss 0.801     \n",
            "Iteration 651000. Loss 0.890     \n",
            "Iteration 652000. Loss 0.806     \n",
            "Iteration 653000. Loss 0.830     \n",
            "Iteration 654000. Loss 0.749     \n",
            "Iteration 655000. Loss 0.861     \n",
            "Iteration 656000. Loss 0.654     \n",
            "Iteration 657000. Loss 0.891     \n",
            "Iteration 658000. Loss 0.769     \n",
            "Iteration 659000. Loss 0.842     \n",
            "Iteration 660000. Loss 0.843     \n",
            "Iteration 661000. Loss 0.885     \n",
            "Iteration 662000. Loss 0.724     \n",
            "Iteration 663000. Loss 0.753     \n",
            "Iteration 664000. Loss 0.783     \n",
            "Iteration 665000. Loss 0.811     \n",
            "Iteration 666000. Loss 0.813     \n",
            "Iteration 667000. Loss 0.887     \n",
            "Iteration 668000. Loss 0.798     \n",
            "Iteration 669000. Loss 0.706     \n",
            "Iteration 670000. Loss 0.710     \n",
            "Iteration 671000. Loss 0.783     \n",
            "Iteration 672000. Loss 0.767     \n",
            "Iteration 673000. Loss 0.809     \n",
            "Iteration 674000. Loss 0.853     \n",
            "Iteration 675000. Loss 0.857     \n",
            "Iteration 676000. Loss 0.907     \n",
            "Iteration 677000. Loss 0.798     \n",
            "Iteration 678000. Loss 0.695     \n",
            "Iteration 679000. Loss 0.881     \n",
            "Iteration 680000. Loss 0.795     \n",
            "Iteration 681000. Loss 0.869     \n",
            "Iteration 682000. Loss 0.675     \n",
            "Iteration 683000. Loss 0.831     \n",
            "Iteration 684000. Loss 0.664     \n",
            "Iteration 685000. Loss 0.842     \n",
            "Iteration 686000. Loss 0.791     \n",
            "Iteration 687000. Loss 0.761     \n",
            "Iteration 688000. Loss 0.802     \n",
            "Iteration 689000. Loss 0.833     \n",
            "Iteration 690000. Loss 0.870     \n",
            "Iteration 691000. Loss 0.837     \n",
            "Iteration 692000. Loss 0.791     \n",
            "Iteration 693000. Loss 0.778     \n",
            "Iteration 694000. Loss 0.773     \n",
            "Iteration 695000. Loss 0.809     \n",
            "Iteration 696000. Loss 0.779     \n",
            "Iteration 697000. Loss 0.804     \n",
            "Iteration 698000. Loss 0.799     \n",
            "Iteration 699000. Loss 0.844     \n",
            "Iteration 700000. Loss 0.861     \n",
            "Iteration 701000. Loss 0.796     \n",
            "Iteration 702000. Loss 0.701     \n",
            "Iteration 703000. Loss 0.842     \n",
            "Iteration 704000. Loss 0.735     \n",
            "Iteration 705000. Loss 0.734     \n",
            "Iteration 706000. Loss 0.854     \n",
            "Iteration 707000. Loss 0.788     \n",
            "Iteration 708000. Loss 0.888     \n",
            "Iteration 709000. Loss 0.896     \n",
            "Iteration 710000. Loss 0.861     \n",
            "Iteration 711000. Loss 0.710     \n",
            "Iteration 712000. Loss 0.828     \n",
            "Iteration 713000. Loss 0.760     \n",
            "Iteration 714000. Loss 0.728     \n",
            "Iteration 715000. Loss 0.797     \n",
            "Iteration 716000. Loss 0.720     \n",
            "Iteration 717000. Loss 0.700     \n",
            "Iteration 718000. Loss 0.821     \n",
            "Iteration 719000. Loss 0.706     \n",
            "Iteration 720000. Loss 0.728     \n",
            "Iteration 721000. Loss 0.652     \n",
            "Iteration 722000. Loss 0.705     \n",
            "Iteration 723000. Loss 0.874     \n",
            "Iteration 724000. Loss 0.826     \n",
            "Iteration 725000. Loss 0.876     \n",
            "Iteration 726000. Loss 0.786     \n",
            "Iteration 727000. Loss 0.807     \n",
            "Iteration 728000. Loss 0.726     \n",
            "Iteration 729000. Loss 0.660     \n",
            "Iteration 730000. Loss 0.739     \n",
            "Iteration 731000. Loss 0.820     \n",
            "Iteration 732000. Loss 0.867     \n",
            "Iteration 733000. Loss 0.809     \n",
            "Iteration 734000. Loss 0.870     \n",
            "Iteration 735000. Loss 0.716     \n",
            "Iteration 736000. Loss 0.764     \n",
            "Iteration 737000. Loss 0.753     \n",
            "Iteration 738000. Loss 0.845     \n",
            "Iteration 739000. Loss 0.805     \n",
            "Iteration 740000. Loss 0.772     \n",
            "Iteration 741000. Loss 0.751     \n",
            "Iteration 742000. Loss 0.730     \n",
            "Iteration 743000. Loss 0.630     \n",
            "Iteration 744000. Loss 0.597     \n",
            "Iteration 745000. Loss 0.664     \n",
            "Iteration 746000. Loss 0.777     \n",
            "Iteration 747000. Loss 0.780     \n",
            "Iteration 748000. Loss 0.938     \n",
            "Iteration 749000. Loss 0.798     \n",
            "Iteration 750000. Loss 0.829     \n",
            "Iteration 751000. Loss 0.875     \n",
            "Iteration 752000. Loss 0.840     \n",
            "Iteration 753000. Loss 0.772     \n",
            "Iteration 754000. Loss 0.769     \n",
            "Iteration 755000. Loss 0.833     \n",
            "Iteration 756000. Loss 0.802     \n",
            "Iteration 757000. Loss 0.717     \n",
            "Iteration 758000. Loss 0.607     \n",
            "Iteration 759000. Loss 0.793     \n",
            "Iteration 760000. Loss 0.683     \n",
            "Iteration 761000. Loss 0.675     \n",
            "Iteration 762000. Loss 0.749     \n",
            "Iteration 763000. Loss 0.817     \n",
            "Iteration 764000. Loss 0.835     \n",
            "Iteration 765000. Loss 0.832     \n",
            "Iteration 766000. Loss 0.823     \n",
            "Iteration 767000. Loss 0.799     \n",
            "Iteration 768000. Loss 0.754     \n",
            "Iteration 769000. Loss 0.751     \n",
            "Iteration 770000. Loss 0.759     \n",
            "Iteration 771000. Loss 0.869     \n",
            "Iteration 772000. Loss 0.745     \n",
            "Iteration 773000. Loss 0.616     \n",
            "Iteration 774000. Loss 0.698     \n",
            "Iteration 775000. Loss 0.738     \n",
            "Iteration 776000. Loss 0.687     \n",
            "Iteration 777000. Loss 0.711     \n",
            "Iteration 778000. Loss 0.846     \n",
            "Iteration 779000. Loss 0.953     \n",
            "Iteration 780000. Loss 0.761     \n",
            "Iteration 781000. Loss 0.762     \n",
            "Iteration 782000. Loss 0.900     \n",
            "Iteration 783000. Loss 0.800     \n",
            "Iteration 784000. Loss 0.733     \n",
            "Iteration 785000. Loss 0.712     \n",
            "Iteration 786000. Loss 0.729     \n",
            "Iteration 787000. Loss 0.848     \n",
            "Iteration 788000. Loss 0.890     \n",
            "Iteration 789000. Loss 0.829     \n",
            "Iteration 790000. Loss 0.880     \n",
            "Iteration 791000. Loss 0.761     \n",
            "Iteration 792000. Loss 0.819     \n",
            "Iteration 793000. Loss 0.797     \n",
            "Iteration 794000. Loss 0.839     \n",
            "Iteration 795000. Loss 0.885     \n",
            "Iteration 796000. Loss 0.840     \n",
            "Iteration 797000. Loss 0.755     \n",
            "Iteration 798000. Loss 0.810     \n",
            "Iteration 799000. Loss 0.763     \n",
            "Iteration 800000. Loss 0.797     \n",
            "Iteration 801000. Loss 0.773     \n",
            "Iteration 802000. Loss 0.765     \n",
            "Iteration 803000. Loss 0.858     \n",
            "Iteration 804000. Loss 0.757     \n",
            "Iteration 805000. Loss 0.777     \n",
            "Iteration 806000. Loss 0.811     \n",
            "Iteration 807000. Loss 0.748     \n",
            "Iteration 808000. Loss 0.860     \n",
            "Iteration 809000. Loss 0.787     \n",
            "Iteration 810000. Loss 0.966     \n",
            "Iteration 811000. Loss 0.818     \n",
            "Iteration 812000. Loss 0.803     \n",
            "Iteration 813000. Loss 0.795     \n",
            "Iteration 814000. Loss 0.746     \n",
            "Iteration 815000. Loss 0.846     \n",
            "Iteration 816000. Loss 0.809     \n",
            "Iteration 817000. Loss 0.712     \n",
            "Iteration 818000. Loss 0.851     \n",
            "Iteration 819000. Loss 0.797     \n",
            "Iteration 820000. Loss 0.814     \n",
            "Iteration 821000. Loss 0.891     \n",
            "Iteration 822000. Loss 0.742     \n",
            "Iteration 823000. Loss 0.722     \n",
            "Iteration 824000. Loss 0.890     \n",
            "Iteration 825000. Loss 0.752     \n",
            "Iteration 826000. Loss 0.613     \n",
            "Iteration 827000. Loss 0.756     \n",
            "Iteration 828000. Loss 0.709     \n",
            "Iteration 829000. Loss 0.794     \n",
            "Iteration 830000. Loss 0.819     \n",
            "Iteration 831000. Loss 0.822     \n",
            "Iteration 832000. Loss 0.786     \n",
            "Iteration 833000. Loss 0.757     \n",
            "Iteration 834000. Loss 0.702     \n",
            "Iteration 835000. Loss 0.906     \n",
            "Iteration 836000. Loss 0.816     \n",
            "Iteration 837000. Loss 0.779     \n",
            "Iteration 838000. Loss 0.774     \n",
            "Iteration 839000. Loss 0.770     \n",
            "Iteration 840000. Loss 0.753     \n",
            "Iteration 841000. Loss 0.689     \n",
            "Iteration 842000. Loss 0.856     \n",
            "Iteration 843000. Loss 0.784     \n",
            "Iteration 844000. Loss 0.865     \n",
            "Iteration 845000. Loss 0.693     \n",
            "Iteration 846000. Loss 0.731     \n",
            "Iteration 847000. Loss 0.756     \n",
            "Iteration 848000. Loss 0.715     \n",
            "Iteration 849000. Loss 0.783     \n",
            "Iteration 850000. Loss 0.943     \n",
            "Iteration 851000. Loss 0.793     \n",
            "Iteration 852000. Loss 0.791     \n",
            "Iteration 853000. Loss 0.907     \n",
            "Iteration 854000. Loss 0.769     \n",
            "Iteration 855000. Loss 0.799     \n",
            "Iteration 856000. Loss 0.684     \n",
            "Iteration 857000. Loss 0.793     \n",
            "Iteration 858000. Loss 0.789     \n",
            "Iteration 859000. Loss 0.826     \n",
            "Iteration 860000. Loss 0.833     \n",
            "Iteration 861000. Loss 0.751     \n",
            "Iteration 862000. Loss 0.740     \n",
            "Iteration 863000. Loss 0.761     \n",
            "Iteration 864000. Loss 0.753     \n",
            "Iteration 865000. Loss 0.610     \n",
            "Iteration 866000. Loss 0.782     \n",
            "Iteration 867000. Loss 0.742     \n",
            "Iteration 868000. Loss 0.747     \n",
            "Iteration 869000. Loss 0.735     \n",
            "Iteration 870000. Loss 0.769     \n",
            "Iteration 871000. Loss 0.856     \n",
            "Iteration 872000. Loss 0.746     \n",
            "Iteration 873000. Loss 0.784     \n",
            "Iteration 874000. Loss 0.746     \n",
            "Iteration 875000. Loss 0.713     \n",
            "Iteration 876000. Loss 0.801     \n",
            "Iteration 877000. Loss 0.853     \n",
            "Iteration 878000. Loss 0.751     \n",
            "Iteration 879000. Loss 0.840     \n",
            "Iteration 880000. Loss 0.725     \n",
            "Iteration 881000. Loss 0.789     \n",
            "Iteration 882000. Loss 0.849     \n",
            "Iteration 883000. Loss 0.764     \n",
            "Iteration 884000. Loss 0.787     \n",
            "Iteration 885000. Loss 0.861     \n",
            "Iteration 886000. Loss 0.790     \n",
            "Iteration 887000. Loss 0.827     \n",
            "Iteration 888000. Loss 0.803     \n",
            "Iteration 889000. Loss 0.851     \n",
            "Iteration 890000. Loss 0.791     \n",
            "Iteration 891000. Loss 0.852     \n",
            "Iteration 892000. Loss 0.733     \n",
            "Iteration 893000. Loss 0.811     \n",
            "Iteration 894000. Loss 0.637     \n",
            "Iteration 895000. Loss 0.741     \n",
            "Iteration 896000. Loss 0.765     \n",
            "Iteration 897000. Loss 0.690     \n",
            "Iteration 898000. Loss 0.706     \n",
            "Iteration 899000. Loss 0.731     \n",
            "Iteration 900000. Loss 0.775     \n",
            "Iteration 901000. Loss 0.799     \n",
            "Iteration 902000. Loss 0.902     \n",
            "Iteration 903000. Loss 0.768     \n",
            "Iteration 904000. Loss 0.684     \n",
            "Iteration 905000. Loss 0.749     \n",
            "Iteration 906000. Loss 0.766     \n",
            "Iteration 907000. Loss 0.708     \n",
            "Iteration 908000. Loss 0.847     \n",
            "Iteration 909000. Loss 0.895     \n",
            "Iteration 910000. Loss 0.747     \n",
            "Iteration 911000. Loss 0.784     \n",
            "Iteration 912000. Loss 0.733     \n",
            "Iteration 913000. Loss 0.763     \n",
            "Iteration 914000. Loss 0.768     \n",
            "Iteration 915000. Loss 0.724     \n",
            "Iteration 916000. Loss 0.717     \n",
            "Iteration 917000. Loss 0.719     \n",
            "Iteration 918000. Loss 0.815     \n",
            "Iteration 919000. Loss 0.809     \n",
            "Iteration 920000. Loss 0.733     \n",
            "Iteration 921000. Loss 0.706     \n",
            "Iteration 922000. Loss 0.710     \n",
            "Iteration 923000. Loss 0.680     \n",
            "Iteration 924000. Loss 0.738     \n",
            "Iteration 925000. Loss 0.734     \n",
            "Iteration 926000. Loss 0.774     \n",
            "Iteration 927000. Loss 0.864     \n",
            "Iteration 928000. Loss 0.870     \n",
            "Iteration 929000. Loss 0.813     \n",
            "Iteration 930000. Loss 0.818     \n",
            "Iteration 931000. Loss 0.780     \n",
            "Iteration 932000. Loss 0.804     \n",
            "Iteration 933000. Loss 0.842     \n",
            "Iteration 934000. Loss 0.832     \n",
            "Iteration 935000. Loss 0.785     \n",
            "Iteration 936000. Loss 0.834     \n",
            "Iteration 937000. Loss 0.744     \n",
            "Iteration 938000. Loss 0.973     \n",
            "Iteration 939000. Loss 0.764     \n",
            "Iteration 940000. Loss 0.767     \n",
            "Iteration 941000. Loss 0.749     \n",
            "Iteration 942000. Loss 0.853     \n",
            "Iteration 943000. Loss 0.772     \n",
            "Iteration 944000. Loss 0.723     \n",
            "Iteration 945000. Loss 0.734     \n",
            "Iteration 946000. Loss 0.710     \n",
            "Iteration 947000. Loss 0.709     \n",
            "Iteration 948000. Loss 0.813     \n",
            "Iteration 949000. Loss 0.782     \n",
            "Iteration 950000. Loss 0.729     \n",
            "Iteration 951000. Loss 0.807     \n",
            "Iteration 952000. Loss 0.904     \n",
            "Iteration 953000. Loss 0.776     \n",
            "Iteration 954000. Loss 0.800     \n",
            "Iteration 955000. Loss 0.759     \n",
            "Iteration 956000. Loss 0.697     \n",
            "Iteration 957000. Loss 0.884     \n",
            "Iteration 958000. Loss 0.885     \n",
            "Iteration 959000. Loss 0.748     \n",
            "Iteration 960000. Loss 0.703     \n",
            "Iteration 961000. Loss 0.622     \n",
            "Iteration 962000. Loss 0.673     \n",
            "Iteration 963000. Loss 0.815     \n",
            "Iteration 964000. Loss 0.767     \n",
            "Iteration 965000. Loss 0.766     \n",
            "Iteration 966000. Loss 0.771     \n",
            "Iteration 967000. Loss 0.753     \n",
            "Iteration 968000. Loss 0.675     \n",
            "Iteration 969000. Loss 0.751     \n",
            "Iteration 970000. Loss 0.852     \n",
            "Iteration 971000. Loss 0.862     \n",
            "Iteration 972000. Loss 0.827     \n",
            "Iteration 973000. Loss 0.725     \n",
            "Iteration 974000. Loss 0.732     \n",
            "Iteration 975000. Loss 0.841     \n",
            "Iteration 976000. Loss 0.880     \n",
            "Iteration 977000. Loss 0.832     \n",
            "Iteration 978000. Loss 0.847     \n",
            "Iteration 979000. Loss 0.869     \n",
            "Iteration 980000. Loss 0.851     \n",
            "Iteration 981000. Loss 0.755     \n",
            "Iteration 982000. Loss 0.859     \n",
            "Iteration 983000. Loss 0.676     \n",
            "Iteration 984000. Loss 0.690     \n",
            "Iteration 985000. Loss 0.706     \n",
            "Iteration 986000. Loss 0.805     \n",
            "Iteration 987000. Loss 0.766     \n",
            "Iteration 988000. Loss 0.736     \n",
            "Iteration 989000. Loss 0.856     \n",
            "Iteration 990000. Loss 0.790     \n",
            "Iteration 991000. Loss 0.819     \n",
            "Iteration 992000. Loss 0.812     \n",
            "Iteration 993000. Loss 0.689     \n",
            "Iteration 994000. Loss 0.746     \n",
            "Iteration 995000. Loss 0.735     \n",
            "Iteration 996000. Loss 0.774     \n",
            "Iteration 997000. Loss 0.812     \n",
            "Iteration 998000. Loss 0.858     \n",
            "Iteration 999000. Loss 0.719     \n",
            "Iteration 1000000. Loss 0.884     \n",
            "Iteration 1001000. Loss 0.745     \n",
            "Iteration 1002000. Loss 0.740     \n",
            "Iteration 1003000. Loss 0.759     \n",
            "Iteration 1004000. Loss 0.847     \n",
            "Iteration 1005000. Loss 0.860     \n",
            "Iteration 1006000. Loss 0.760     \n",
            "Iteration 1007000. Loss 0.831     \n",
            "Iteration 1008000. Loss 0.822     \n",
            "Iteration 1009000. Loss 0.866     \n",
            "Iteration 1010000. Loss 0.817     \n",
            "Iteration 1011000. Loss 0.717     \n",
            "Iteration 1012000. Loss 0.653     \n",
            "Iteration 1013000. Loss 0.769     \n",
            "Iteration 1014000. Loss 0.788     \n",
            "Iteration 1015000. Loss 0.716     \n",
            "Iteration 1016000. Loss 0.781     \n",
            "Iteration 1017000. Loss 0.696     \n",
            "Iteration 1018000. Loss 0.741     \n",
            "Iteration 1019000. Loss 0.688     \n",
            "Iteration 1020000. Loss 0.666     \n",
            "Iteration 1021000. Loss 0.745     \n",
            "Iteration 1022000. Loss 0.693     \n",
            "Iteration 1023000. Loss 0.694     \n",
            "Iteration 1024000. Loss 0.849     \n",
            "Iteration 1025000. Loss 0.920     \n",
            "Iteration 1026000. Loss 0.756     \n",
            "Iteration 1027000. Loss 0.828     \n",
            "Iteration 1028000. Loss 0.727     \n",
            "Iteration 1029000. Loss 0.769     \n"
          ]
        }
      ]
    }
  ]
}